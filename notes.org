#+PROPERTY: header-args :exports code :eval no-export 
#+PROPERTY: header-args:scheme :tangle "gwl-gladiator.scm" 
#+PROPERTY: header-args:nextflow :tangle "gladiator.nf" :comments link
#+PROPERTY: header-args:dot :output-dir img :file-ext (pcase org-export-current-backend ('latex "png") (_ "svg")) :eval yes :exports results
#+TODO: TODO(t) | DONE(d)
#+OPTIONS: ^:{} todo:nil H:20
#+LINK: gladiator file:../glaDIAtor/%s
* About this document
This is a literate programming (org-mode) document that describes the
the nextflow implemantiation of gladiator (https://github.com/elolab/glaDIAtor)
and all needed template files.
This directory should already contain the tangled files.
To learn more about org-mode, see https://orgmode.org/.

** Writing Style 
Sentences with the first person plural ("we") as subject
or with implied third person (it reads as "[The program] ..."),
are notes about the development process or an explanation of the program,
whereas sentences with the  second person as object ("You" e.g. "You might try setting foo to 3")
are instructions to the end user.


** Tangling
in order to turn this file into the needed files run
#+begin_src sh 
yes '//' | emacs --batch -Q --file notes.org -f org-babel-tangle 
#+end_src

The following is a list of all files this document tangles into
#+begin_src emacs-lisp :exports results :tangle no
(mapcar 'list (mapcar 'car (org-babel-tangle-collect-blocks)))
#+end_src

#+RESULTS:
| gwl-gladiator.scm                      |
| gladiator.nf                           |
| diaumpireconfig.txt                    |
| comet_template.txt                     |
| xtandem-template.xml                   |
| tpp-5.2-fix.diff                       |
| irt.txt                                |
| pyprophet-legacy-requirements.txt      |
| pyprophet-legacy-standalone.dockerfile |
| install-R-packages.R                   |
| config/singularity.nf                  |
| config/singularity-local.nf            |
| config/docker.nf                       |
| config/docker-local.nf                 |
| config/podman.nf                       |
| config/podman-local.nf                 |
| nextflow.config                        |
| nextflow.tags                          |



* Overview of this pipeline
# hacky space between node and [ for fontlocking
#+NAME: flow
#+begin_src dot 
digraph flow
{
  rankdir=UD
  compound=true
  subgraph processes
  {
    node[shape=square]
    buildfastadatabase[label="Build\nFasta\nDatabase"]
    comet[label="Comet"]
    comet_and_tandem[label="Combine"]
    tandem
    diaumpire[label="Generate\n Pseudospectra"]
    spectrast
    find_swath_windows[label="Finding\nSwath\nwindows"]
    create_transitions[label="Create\nTransitions"]
    osw
    pyprophet_legacy[label="Pyprophet"]
    feature_alignment[label="Feature\nAlignment"]
    swath2stats
  }
  subgraph data{
    node[shape=circle]
    subgraph inputs {
      rank=same 
      fasta[label="Fasta\nFiles"]
      dda
      dia
    }
  }
  
  subgraph cluster_spectra
  {
    label="Spectra"
    style=dashed
    diaumpire
    dda
    choose_spectra[shape=triangle,label="or"]
  }

  
  subgraph cluster_spectral_libgen{
    label="Creating The Spectral Library";
    style=dashed;
    // searching_inputs[style=invis,shape=point,fixedsize=true,label="",size=0,shape=point,height=0,width=0]
    subgraph cluster_searching {
      label="Searching"
      style=dashed
      comet
      tandem
      {comet tandem}->comet_and_tandem
    }
    spectrast
  }

  // searching_inputs -> {tandem comet}[ltail=cluster_searching]
  subgraph cluster_diapart {
    // edge[constraint=false]
    style=dashed
    label="DIA Analysis"
    create_transitions
    osw
    find_swath_windows
    pyprophet_legacy
    feature_alignment
    swath2stats
  }

  // choose_spectra->searching_inputs[lhead=cluster_searching ,ltail=cluster_spectra]
  // buildfastadatabase->searching_inputs[lhead=cluster_searching]
  
  // these are the per-sample steps
  {
    edge[style="dashed,bold"]
    { choose_spectra } -> {comet tandem}
    { dia -> diaumpire }
    diaumpire->choose_spectra
    dda->choose_spectra
    dia ->osw
  }

  // these are the all-samples-combined-steps
  {
    spectrast->create_transitions
    dia->find_swath_windows
    comet_and_tandem -> spectrast
    buildfastadatabase -> spectrast

    fasta->buildfastadatabase
    osw -> pyprophet_legacy -> feature_alignment -> swath2stats
  }
  // these are steps that are not related to samples
  {
    { buildfastadatabase -> {comet tandem} }
    find_swath_windows->{osw,create_transitions}
    create_transitions -> osw
  }

}
#+end_src



* Preprocessing Data
We will not distribute the vendored msconvert,
but if you have DDA-files you need to convert froma propriatry format, to mzmxml,
following the picking peaks step,
and you can use the docker image of =dockerhub:chambm/pwiz-skyline-i-agree-to-the-vendor-licenses=.
You can convert your DIA-files with the same container following "Converting Dia Raw with Msconvert"
** Picking Peaks
#+begin_src sh 
mkdir -p MZXML-pwiz
for f in RAW/*.wiff; do
    wine qtofpeakpicker --resolution=2000 --area=1 --threshold=1 --smoothwidth=1.1 --in $f --out MZXML-pwiz/$(basename --suffix=.wiff $f).mzXML
done
#+end_src
** Converting Dia Raw with Msconvert
#+begin_src sh
mkdir -p MZML-pwiz
find . -iname '*.wiff' -print0 | xargs -P5 -0 -i wine msconvert {} --filter 'titleMaker <RunId>.<ScanNumber>.<ScanNumber>.<ChargeState> File:"<SourcePath>", NativeID:"<Id>"' -o MZML-pwiz/
#+end_src


* Analysis [5/5]
Gladiator paper: doi:10.1038/s43705-022-00137-0
diatools: doi:10.1021/acs.jproteome.9b00606
** Headers
#+begin_src emacs-lisp :tangle no
(setq org-babel-tangle-lang-exts
      (cl-remove-duplicates 
      (append
       '(("scheme" . "scm"))
       org-babel-tangle-lang-exts
       )
      :test 'equal))
#+end_src

#+RESULTS:
: ((scheme . scm) (python . py) (D . d) (C++ . cpp) (emacs-lisp . el) (elisp . el))
#+NAME: gwl-header-block
#+begin_src scheme :noweb no-export
(define-module (workflow)
  #:use-module (gwl workflows)
  #:use-module (gwl processes)
  #:use-module (gwl utils)
  #:use-module (gwl sugar))

<<gwl-vars>>
#+end_src

#+NAME: nf-header-block
#+begin_src nextflow :noweb no-export 
<<nf-vars>>
#+end_src
** DONE building database
*Overview*
[[file:glaDIAtor/workflow.py::def build_database(\\][build_database definition]]

#+NAME: dot-build-database
#+begin_src dot 
digraph {
	cat[shape=box,label="Cat with Bio.SeqIo"]
	DecoyDatabase[shape=box, ]
	DB_fasta[style=bold,label="DB.fasta"]
	DB_with_decoy[style=bold,label="DB_with_decoys.fasta"]
	all_fasta_files -> cat [label="cannot be parallized"]
	cat -> DB_fasta -> DecoyDatabase -> DB_with_decoy
}
#+end_src


*** Combining Fasta Files
#+NAME: py-joinfastafiles
#+begin_src python :tangle no
from Bio import SeqIO
def join_fasta_files(input_files, output_file):
    IDs = set()
    seqRecords = []
    for filename in input_files:
        records = SeqIO.index(filename, "fasta")
        for ID in records:
            if ID not in IDs:
                seqRecords.append(records[ID])
                IDs.add(ID)
            else:
                print("Found duplicated sequence ID " + str(ID) + ", skipping this sequence from file " + filename)

    SeqIO.write(seqRecords, output_file, "fasta")
#+end_src

#+NAME: nf-joinfastafiles
#+begin_src nextflow :noweb no-export
process JoinFastaFiles {
    input:
    file fasta_files from fasta_files_ch.toSortedList()
    output:
    file 'joined_database.fasta' into joined_fasta_database_ch

    """
    #!/usr/bin/env python3
    <<py-joinfastafiles>>
    join_fasta_files("$fasta_files".split(" "), 'joined_database.fasta')
    """
}
#+end_src

#+begin_src nextflow :noweb-ref nf-params :tangle no
params.fastafiles='fasta/*.fasta'
#+end_src


#+begin_src nextflow :noweb-ref nf-vars :tangle no
Channel.fromPath(params.fastafiles).set{fasta_files_ch}
#+end_src

This was how we could set the fasta_files_ch to be in the same order as
the original bruderer run
#+begin_src nextflow :tangle no
Channel.from([
    "fasta/Q7M135.fasta",
    "fasta/irtfusion.fasta",
    "fasta/trypsin.fasta",
    "fasta/uniprot_human_2017_04_05.fasta",
    "fasta/Bruderer_QS-spike-in-proteins.fasta"])
    .map{file(it)}
    .set({fasta_files_ch})
#+end_src
#+NAME: gwl-joinfastafiles-deps
#+begin_src scheme :noweb-ref deps :tangle no
("join-fasta-files"
 "python"
 "biopython")
#+end_src

#+NAME: gwl-joinfastafiles
#+begin_src scheme :noweb no-export
(define (join-fasta-files fasta-files)
  (make-process
   (name "join-fasta-files")
   (synopsis "Join fasta files into one file")
   (packages
    (cdr (quote
      <<gwl-joinfastafiles-deps>>)))
   (inputs (files fasta-files))
   (outputs "joined-fasta.fasta")
   # python
{
<<py-joinfastafiles>>
join_fasta_files({{inputs}}.split(" "),{{outputs}})
}))
#+end_src

#+begin_src scheme :noweb-ref gwl-vars :tangle no
(define fasta-files
  '("Q7M135.fasta" "trypsin.fasta"))
#+end_src

#+begin_src scheme :noweb-ref gwl-proc :tangle no
(join-fasta-files fasta-files)
#+end_src
*** Adding Decoys
#+NAME: nf-buildfastadatabase
#+begin_src nextflow
fasta_db_with_decoys = Channel.value()
process BuildFastaDatabase {
    input:
    file joined_fasta_db from joined_fasta_database_ch
    output:
    file "DB_with_decoys.fasta" into joined_fasta_with_decoys_ch
    """
    DecoyDatabase -in $joined_fasta_db -out DB_with_decoys.fasta
    """
}
#+end_src
=DecoyDatabase= package is from =OpenMs/utils=
https://abibuilder.informatik.uni-tuebingen.de/archive/openms/Documentation/release/latest/html/UTILS_DecoyDatabase.html
https://github.com/OpenMS/OpenMS.git
License: BSD-3 clause
(Not in guix, but uses cmake as build-program,
should be relatively easy to define)
#+NAME: gwl-buildfastadatabase
#+begin_src scheme 
(define create-database-with-decoys
  (make-process
   (name "create-database-with-database")
   (synopsis "Add decoys to fasta database")
   (inputs "joined-fasta.fasta")
   (outputs "DB-with-decoys.fasta")
   (packages )
   # sh
     {
      DecoyDatabase -in $inputs -out $outputs
		    })))
#+end_src

#+begin_src scheme :noweb-ref gwl-proc :tangle no
create-database-with-decoys
#+end_src
** DONE Creating Pseudospectra
:LOGBOOK:
CLOCK: [2022-06-07 Tue 10:33]--[2022-06-07 Tue 19:38] =>  9:05
:END:
[[file:glaDIAtor/workflow.py::def runDiaumpire]]
#+NAME: dot-create-pseudospectra
#+begin_src dot :file-ext svg
digraph {
    DIA[label="DIA_1.mzXML"]
    diaumpire[shape="box",label="java -jar DiaUmpire.jar"]
    pseudo_spectra[label="libfree/DIA_1.mgs"]
    diaumpire_cfg[label="diaumpirecfg.txt (singleton)"]
    pseudo_spectra_mzxml[label="libfree-pseudospectra/DIA_1.mzXML"]
		   
    DIA -> diaumpire
    diaumpire_cfg -> diaumpire
    diaumpire -> pseudo_spectra -> { msconvert[shape="box"] } -> pseudo_spectra_mzxml
}
#+end_src

#+RESULTS: dot-create-pseudospectra
[[file:img/dot-create-pseudospectra.svg]]
https://github.com/Nesvilab/DIA-Umpire/tree/master
*** Problems you might encounter during this step
**** Out of Memory in Dia-umpire
Dia-umpire, which we use here for pseudo-spectra creation,
has pretty extreme memory requirements,
in  your config file you can set the process specific memory (required to be in Gigabyes) e.g. 
#+begin_src nextflow :tangle no :eval no 
process { 
  withName: 'GeneratePseudoSpectra'
  {
	time='96h'
	memory='400 GB'
  }
}
#+end_src
see also [[https://www.nextflow.io/docs/latest/process.html#memory][The Nextflow documentation about process memory]]
**** =MzmlToMzxml= processigng error.
If you get an error of
#+begin_src fundamental :tangle no
  processing file: RD139_Narrow_UPS1_50fmol_inj3.mzML
  [SpectrumList_mzML::create()] Bad istream.
  Error processing file RD139_Narrow_UPS1_50fmol_inj3.mzML
#+end_src
in =MzmltoMzxml=, that can mean that something went wrong
when you used =msconvert= to convert from the propriatary format to mzml
*** Steps that are run 
DIAumpire is =Apache 2= licensed.
#+NAME: diaumpireconfig
#+begin_src conf :eval no :tangle diaumpireconfig.txt 
#Number of threads
# set to the number of cores available
# In the original gladiator, this was set by replicing this all caps 
Thread = 4

#Precursor-fragments grouping parameters
RPmax = 25
RFmax = 300
CorrThreshold = 0.2
DeltaApex = 0.6
RTOverlap = 0.3

#Fragment intensity adjustments
# change BoostComplementaryIon if later using database search results to build libraries for Skyline/OpenSWATH
AdjustFragIntensity = true
BoostComplementaryIon = true

#Export detected MS1 features (output feature file can be loaded and mapped to RAW data in BatMass)
ExportPrecursorPeak = false

#Signal extraction: mass accuracy and resolution
# resolution parameter matters only for data generated in profile mode
SE.MS1PPM = 15
SE.MS2PPM = 25
SE.Resolution = 60000

#Signal extraction: signal to noise filter
SE.SN = 1.1
SE.MS2SN = 1.1

#Signal extraction: minimum signal intensity filter
# for Thermo data, filtering is usually not necessary. Set SE.EstimateBG to false and SE.MinMSIntensity and SE.MinMSMSIntensity to a low value, e.g. 1
# for older Q Exactive data, or when too many MS1 features are extracted, set SE.EstimateBG to yes (or apply SE.MinMSIntensity and SE.MinMSMSIntensity values based on BatMass visualization)
SE.EstimateBG = false
SE.MinMSIntensity = 1
SE.MinMSMSIntensity = 1

#Signal extraction: peak curve detection and isotope grouping
# for older Q Exactive data, or when too many MS1 features are extracted, set SE.NoMissedScan to 1
SE.NoMissedScan = 2
SE.MaxCurveRTRange = 2
SE.RemoveGroupedPeaks = true
SE.RemoveGroupedPeaksRTOverlap = 0.3
SE.RemoveGroupedPeaksCorr = 0.3
SE.MinNoPeakCluster = 2
SE.MaxNoPeakCluster = 4

#Signal extraction: filtering of MS1 features 
# if interested in modified peptides, increase MassDefectOffset parameter, or set SE.MassDefectFilter to false
SE.IsoPattern = 0.3
SE.MassDefectFilter = true
SE.MassDefectOffset = 0.1

#Signal extraction: other 
SE.StartCharge = 1
SE.EndCharge = 5
SE.MS2StartCharge = 2
SE.MS2EndCharge = 5
SE.MinFrag=10
SE.StartRT = 0
SE.EndRT = 9999
SE.MinMZ = 200
SE.MinPrecursorMass = 600
SE.MaxPrecursorMass = 5000


#Isolation window setting
#The current version supports the following window type: SWATH (fixed window size), V_SWATH (variable SWATH window), MSX, MSE, pSMART
WindowType=SWATH

#Fix window size (For SWATH)
# for Thermo data, this will be determined from raw data automatically
#WindowSize=15

#Variable SWATH window setting (start m/z, end m/z, separated by Tab)
# for Thermo data, this will be determined from raw data automatically

#==window setting begin
#==window setting end
#+end_src
MGF = Mascot Generic Format
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3518119/
#+begin_src nextflow
// create mzxml
process MzmlToMzxml {
    input:
    file diafile from dia_mzml_files_for_pseudospectra_ch
    output:
    file "*.mzXML" into dia_mzxml_files_for_pseudospectra_ch
    """
    msconvert $diafile --32 --zlib --filter "peakPicking false 1-" --mzXML
    """
        
}

process GeneratePseudoSpectra  {
    memory '16 GB' 
    input:
    file diafile from dia_mzxml_files_for_pseudospectra_ch
    path diaumpireconfig from diaumpireconfig_ch.first()
    output:
    // we flatten here becuase a single mzxml might result in multiple mgf files
    file "*.mgf" into pseudospectra_mgf_ch mode flatten 

    """
    # we set \$1 to the number of gigs of memory
    set -- $task.memory
    java -Xmx\$1g -Xms\$1g -jar /opt/dia-umpire/DIA_Umpire_SE.jar $diafile $diaumpireconfig
    """
}

process MgfToMzxml {
    input:
    file mgf from pseudospectra_mgf_ch
    output:
    file "*.mzXML" into pseudospectra_ch
    
    """
    msconvert $mgf --mzXML 
    """
}
#+end_src


#+begin_src scheme :noweb-ref deps :tangle no 
("generate-pseudo-spectra"
 "dia-umpire" 
 "pwiz") ;; the free one 
#+end_src


though this  might also be done with openms's =FileConverter= ? which is more conventionally build 
https://abibuilder.informatik.uni-tuebingen.de/archive/openms/Documentation/release/latest/html/TOPP_FileConverter.html
mstools
#+begin_src nextflow :noweb-ref nf-params :tangle no
params.diaumpireconfig='diaumpireconfig.txt'
params.diafiles = "DIA/*.mzML"
params.ddafiles = "DDA/*.mzXML"
params.dda_assisted = false
#+end_src

#+begin_src nextflow :noweb-ref nf-vars :tangle no
// so that this is a singleton channel
diaumpireconfig_ch = Channel.fromPath(params.diaumpireconfig)
Channel.fromPath(params.ddafiles).into{dda_files_ch ; dda_files_empty_check}
// dda_assisted_p = dda_files_empty_check.ifEmpty('Empty').first() != 'Empty'
dda_assisted_p = params.dda_assisted

(initial_dia_mzml_files_ch, dia_mzml_files_for_pseudospectra_ch) =
  (dda_assisted_p ?
    [ Channel.fromPath(params.diafiles), Channel.empty() ] :
   // take(2) for development short cycle
  Channel.fromPath(params.diafiles).into(2)
  // Channel.fromPath(params.diafiles).into(2)
)

initial_dia_mzml_files_ch.multiMap{
    it -> swath_windows: osw: it
    }.set{dia_mzml_files_ch}
    
#+end_src
As dia will be listened to by two in the case of not dda-assisted,
we'll have to use the [[https://www.nextflow.io/docs/latest/operator.html#operator-into][=into=]] operator to input into two channels if we dont have =dda=  data


The channel-juggling below  makes the path for pseudo-spectra and "real" input spectra
follow the same path after =MgfToMzxml= / from comet&xtandem onwards,
by putting the ones to act on in the =maybespectra_ch.{comet,xtandem}= channels.
#+begin_src nextflow :noweb-ref nf-vars :tangle no
// we forward-declare pseudospectra_ch here (output of the MgfToMzxml process)
// so that we can define a multiMap rule for it.
pseudospectra_ch = Channel.create()
// maybe-spectra can be either real spectra or pseudo-spectra
if (dda_assisted_p){
    dda_files_ch.multiMap { it ->
	spectrast: comet: xtandem: it }.set{maybespectra_ch}
} else {
    pseudospectra_ch.multiMap { it ->
	spectrast: comet: xtandem: it }.set{maybespectra_ch}
}
#+end_src 


** DONE Creating Swath window files
:PROPERTIES:
:ID:       1469cbd6-2fe8-4919-8808-85f17dfee228
:END:
[[file:glaDIAtor/workflow.py::def create_swath_window_files]]
outputs files ~swath-windows.txt~,
~truncated-swath-windows.txt~

#+NAME: py-makeswathwindows
#+begin_src python :tangle no
import xml.etree.ElementTree as ET
import os

def read_swath_windows(dia_mzML):

    print ("DEBUG: reading_swath_windows: ", dia_mzML)
    
    context = ET.iterparse(dia_mzML, events=("start", "end"))

    windows = {}
    for event, elem in context:

        if event == "end" and elem.tag == '{http://psi.hupo.org/ms/mzml}precursor':
            il_target = None
            il_lower = None
            il_upper = None

            isolationwindow = elem.find('{http://psi.hupo.org/ms/mzml}isolationWindow')
            for cvParam in isolationwindow.findall('{http://psi.hupo.org/ms/mzml}cvParam'):
                name = cvParam.get('name')
                value = cvParam.get('value')

                if (name == 'isolation window target m/z'):
                    il_target = value
                elif (name == 'isolation window lower offset'):
                    il_lower = value
                elif (name == 'isolation window upper offset'):
                    il_upper = value

            ionList = elem.find('{http://psi.hupo.org/ms/mzml}selectedIonList')
           
            selectedion = ionList.find('{http://psi.hupo.org/ms/mzml}selectedIon')

            if selectedion:
            
                for cvParam in selectedion.findall('{http://psi.hupo.org/ms/mzml}cvParam'):
                    name = cvParam.get('name')
                    value = cvParam.get('value')

                    if (name == 'selected ion m/z'):
                        if not il_target:
                            il_target = value
                
            if not il_target in windows:
                windows[il_target] = (il_lower, il_upper)
            else:
                lower, upper = windows[il_target]
                assert (il_lower == lower)
                assert (il_upper == upper)
                return windows

    return windows

def create_swath_window_files(cwd, dia_mzML):

    windows = read_swath_windows(dia_mzML)

    swaths = []
    for x in windows:
        target_str = x
        lower_str, upper_str = windows[x]
        target = float(target_str)
        lower = float(lower_str)
        upper = float(upper_str)
        assert (lower > 0)
        assert (upper > 0)
        swaths.append((target - lower, target + upper))
        
    swaths.sort(key=lambda tup: tup[0])

    tswaths = []
    tswaths.append(swaths[0])
    for i in range(1, len(swaths)):
        if swaths[i-1][1] > swaths[i][0]:
            lower_prev, upper_prev = swaths[i-1]
            lower, upper = swaths[i]
            assert (upper_prev < upper)
            tswaths.append((upper_prev, upper))
        else:
            tswaths.append(swaths[i])

    assert (len(swaths) == len(tswaths))
    # here we use chr(10) (equivalent to slash n), and chr(9) (equivalent to slash t)  because i dont wanna deal with nextflow headaches
    newline_character = chr(10)
    tab_character = chr(9)
    with open(os.path.join(cwd, "swath-windows.txt"), "w") as fh_swaths, open(os.path.join(cwd, "truncated-swath-windows.txt"), "w") as fh_tswaths:

        fh_tswaths.write("LowerOffset"+tab_character+"HigherOffset"+newline_character)

        for i in range(len(swaths)): 
            fh_swaths.write(str(swaths[i][0]) + tab_character + str(swaths[i][1])  + newline_character)
            fh_tswaths.write(str(tswaths[i][0]) + tab_character + str(tswaths[i][1])  + newline_character)

    return swaths, tswaths

#+end_src

#+begin_src nextflow  :noweb no-export
process MakeSwathWindows {
    input:
    file diafile from dia_mzml_files_ch.swath_windows.first()
    output: 
    file "swath-windows.txt" into  swath_windows_ch
    file "truncated-swath-windows.txt" into truncated_swath_windows_ch
    
    shell:
    '''
    #!/usr/bin/env python3
    <<py-makeswathwindows>>
    swaths, _  = create_swath_window_files(".","!{diafile}")
    '''
    
}

#+end_src

we'll have to get minswath and maxswath by reading ~"swath-windows.txt"~

** DONE Building {Pseudo-,}Spectral library  [4/4]
*** Choosing Comet/Xtandem
:LOGBOOK:
CLOCK: [2022-06-10 Fri 10:20]--[2022-06-10 Fri 11:00] =>  0:40
:END:
#+begin_src nextflow :noweb-ref nf-params :tangle no
params.search_engines = ["comet","xtandem"]

#+end_src
*** DONE Comet 
:LOGBOOK:
CLOCK: [2022-06-10 Fri 13:56]--[2022-06-10 Fri 16:09] =>  2:13
CLOCK: [2022-06-10 Fri 11:00]--[2022-06-10 Fri 13:31] =>  2:31
CLOCK: [2022-06-09 Thu 09:31]--[2022-06-09 Thu 09:31] =>  0:00
CLOCK: [2022-06-08 Wed 15:31]--[2022-06-08 Wed 17:51] =>  2:20
CLOCK: [2022-06-08 Wed 13:00]--[2022-06-08 Wed 14:24] =>  1:24
CLOCK: [2022-06-08 Wed 09:32]--[2022-06-08 Wed 11:48] =>  2:16
CLOCK: [2022-06-08 Wed 08:48]--[2022-06-08 Wed 09:32] =>  0:44
:END:
file:glaDIAtor/UI/ui/__init__.py::workflow.runComet
[[file:glaDIAtor/workflow.py::def runComet]]
#+begin_src conf :tangle comet_template.txt
# comet_version 2019.01 rev. 5
# Comet MS/MS search engine parameters file.
# Everything following the '#' symbol is treated as a comment.

database_name = @DDA_DB_FILE@
decoy_search = 0                       # 0=no (default), 1=concatenated search, 2=separate search
peff_format = 0                        # 0=no (normal fasta, default), 1=PEFF PSI-MOD, 2=PEFF Unimod
peff_obo =                             # path to PSI Mod or Unimod OBO file

num_threads = 0                        # 0=poll CPU to set num threads; else specify num threads directly (max 128)

#
# masses
#
peptide_mass_tolerance = @PRECURSOR_MASS_TOLERANCE@
peptide_mass_units = 2                 # 0=amu, 1=mmu, 2=ppm
mass_type_parent = 1                   # 0=average masses, 1=monoisotopic masses
mass_type_fragment = 1                 # 0=average masses, 1=monoisotopic masses
precursor_tolerance_type = 1           # 0=MH+ (default), 1=precursor m/z; only valid for amu/mmu tolerances
isotope_error = 3                      # 0=off, 1=0/1 (C13 error), 2=0/1/2, 3=0/1/2/3, 4=-8/-4/0/4/8 (for +4/+8 labeling)

#
# search enzyme
#
search_enzyme_number = 1               # choose from list at end of this params file
search_enzyme2_number = 0              # second enzyme; set to 0 if no second enzyme
num_enzyme_termini = 2                 # 1 (semi-digested), 2 (fully digested, default), 8 C-term unspecific , 9 N-term unspecific
allowed_missed_cleavage = 1            # maximum value is 5; for enzyme search

#
# Up to 9 variable modifications are supported
# format:  <mass> <residues> <0=variable/else binary> <max_mods_per_peptide> <term_distance> <n/c-term> <required> <neutral_loss>
#l     e.g. 79.966331 STY 0 3 -1 0 0 97.976896
#
variable_mod01 = 15.9949 M 0 3 -1 0 0 0.0
variable_mod02 = 0.0 X 0 3 -1 0 0 0.0
variable_mod03 = 0.0 X 0 3 -1 0 0 0.0
variable_mod04 = 0.0 X 0 3 -1 0 0 0.0
variable_mod05 = 0.0 X 0 3 -1 0 0 0.0
variable_mod06 = 0.0 X 0 3 -1 0 0 0.0
variable_mod07 = 0.0 X 0 3 -1 0 0 0.0
variable_mod08 = 0.0 X 0 3 -1 0 0 0.0
variable_mod09 = 0.0 X 0 3 -1 0 0 0.0
max_variable_mods_in_peptide = 5
require_variable_mod = 0

#
# fragment ions
#
# ion trap ms/ms:  1.0005 tolerance, 0.4 offset (mono masses), theoretical_fragment_ions = 1
# high res ms/ms:    0.02 tolerance, 0.0 offset (mono masses), theoretical_fragment_ions = 0, spectrum_batch_size = 15000
#
fragment_bin_tol = @FRAGMENT_MASS_TOLERANCE@              # binning to use on fragment ions
fragment_bin_offset = 0.0              # offset position to start the binning (0.0 to 1.0)
theoretical_fragment_ions = 1          # 0=use flanking peaks, 1=M peak only
use_A_ions = 0
use_B_ions = 1
use_C_ions = 0
use_X_ions = 0
use_Y_ions = 1
use_Z_ions = 0
use_NL_ions = 0                        # 0=no, 1=yes to consider NH3/H2O neutral loss peaks

#
# output
#
output_sqtstream = 0                   # 0=no, 1=yes  write sqt to standard output
output_sqtfile = 0                     # 0=no, 1=yes  write sqt file
output_txtfile = 0                     # 0=no, 1=yes  write tab-delimited txt file
output_pepxmlfile = 1                  # 0=no, 1=yes  write pep.xml file
output_percolatorfile = 1              # 0=no, 1=yes  write Percolator tab-delimited input file
print_expect_score = 1                 # 0=no, 1=yes to replace Sp with expect in out & sqt
num_output_lines = 5                   # num peptide results to show
show_fragment_ions = 0                 # 0=no, 1=yes for out files only

sample_enzyme_number = 1               # Sample enzyme which is possibly different than the one applied to the search.
                                       # Used to calculate NTT & NMC in pepXML output (default=1 for trypsin).

#
# mzXML parameters
#
scan_range = 0 0                       # start and end scan range to search; either entry can be set independently
precursor_charge = 0 0                 # precursor charge range to analyze; does not override any existing charge; 0 as 1st entry ignores parameter
override_charge = 0                    # 0=no, 1=override precursor charge states, 2=ignore precursor charges outside precursor_charge range, 3=see online
ms_level = 2                           # MS level to analyze, valid are levels 2 (default) or 3
activation_method = HCD                # activation method; used if activation method set; allowed ALL, CID, ECD, ETD, PQD, HCD, IRMPD

#
# misc parameters
#
digest_mass_range = 600.0 5000.0       # MH+ peptide mass range to analyze
peptide_length_range = 5 63            # minimum and maximum peptide length to analyze (default 1 63; max length 63)
num_results = 100                      # number of search hits to store internally
max_duplicate_proteins = 20            # maximum number of protein names to report for each peptide identification; -1 reports all duplicates
skip_researching = 1                   # for '.out' file output only, 0=search everything again (default), 1=don't search if .out exists
max_fragment_charge = 3                # set maximum fragment charge state to analyze (allowed max 5)
max_precursor_charge = 6               # set maximum precursor charge state to analyze (allowed max 9)
nucleotide_reading_frame = 0           # 0=proteinDB, 1-6, 7=forward three, 8=reverse three, 9=all six
clip_nterm_methionine = 0              # 0=leave sequences as-is; 1=also consider sequence w/o N-term methionine
spectrum_batch_size = 15000            # max. # of spectra to search at a time; 0 to search the entire scan range in one loop
decoy_prefix = DECOY_                  # decoy entries are denoted by this string which is pre-pended to each protein accession
equal_I_and_L = 1                      # 0=treat I and L as different; 1=treat I and L as same
output_suffix =                        # add a suffix to output base names i.e. suffix "-C" generates base-C.pep.xml from base.mzXML input
mass_offsets =                         # one or more mass offsets to search (values substracted from deconvoluted precursor mass)
precursor_NL_ions =                    # one or more precursor neutral loss masses, will be added to xcorr analysis

#
# spectral processing
#
minimum_peaks = 10                     # required minimum number of peaks in spectrum to search (default 10)
minimum_intensity = 0                  # minimum intensity value to read in
remove_precursor_peak = 0              # 0=no, 1=yes, 2=all charge reduced precursor peaks (for ETD), 3=phosphate neutral loss peaks
remove_precursor_tolerance = 1.5       # +- Da tolerance for precursor removal
clear_mz_range = 0.0 0.0               # for iTRAQ/TMT type data; will clear out all peaks in the specified m/z range

#
# additional modifications
#

add_Cterm_peptide = 0.0
add_Nterm_peptide = 0.0
add_Cterm_protein = 0.0
add_Nterm_protein = 0.0

add_G_glycine = 0.0000                 # added to G - avg.  57.0513, mono.  57.02146
add_A_alanine = 0.0000                 # added to A - avg.  71.0779, mono.  71.03711
add_S_serine = 0.0000                  # added to S - avg.  87.0773, mono.  87.03203
add_P_proline = 0.0000                 # added to P - avg.  97.1152, mono.  97.05276
add_V_valine = 0.0000                  # added to V - avg.  99.1311, mono.  99.06841
add_T_threonine = 0.0000               # added to T - avg. 101.1038, mono. 101.04768
add_C_cysteine = 57.021464             # added to C - avg. 103.1429, mono. 103.00918
add_L_leucine = 0.0000                 # added to L - avg. 113.1576, mono. 113.08406
add_I_isoleucine = 0.0000              # added to I - avg. 113.1576, mono. 113.08406
add_N_asparagine = 0.0000              # added to N - avg. 114.1026, mono. 114.04293
add_D_aspartic_acid = 0.0000           # added to D - avg. 115.0874, mono. 115.02694
add_Q_glutamine = 0.0000               # added to Q - avg. 128.1292, mono. 128.05858
add_K_lysine = 0.0000                  # added to K - avg. 128.1723, mono. 128.09496
add_E_glutamic_acid = 0.0000           # added to E - avg. 129.1140, mono. 129.04259
add_M_methionine = 0.0000              # added to M - avg. 131.1961, mono. 131.04048
add_O_ornithine = 0.0000               # added to O - avg. 132.1610, mono  132.08988
add_H_histidine = 0.0000               # added to H - avg. 137.1393, mono. 137.05891
add_F_phenylalanine = 0.0000           # added to F - avg. 147.1739, mono. 147.06841
add_U_selenocysteine = 0.0000          # added to U - avg. 150.0379, mono. 150.95363
add_R_arginine = 0.0000                # added to R - avg. 156.1857, mono. 156.10111
add_Y_tyrosine = 0.0000                # added to Y - avg. 163.0633, mono. 163.06333
add_W_tryptophan = 0.0000              # added to W - avg. 186.0793, mono. 186.07931
add_B_user_amino_acid = 0.0000         # added to B - avg.   0.0000, mono.   0.00000
add_J_user_amino_acid = 0.0000         # added to J - avg.   0.0000, mono.   0.00000
add_X_user_amino_acid = 0.0000         # added to X - avg.   0.0000, mono.   0.00000
add_Z_user_amino_acid = 0.0000         # added to Z - avg.   0.0000, mono.   0.00000

#
# COMET_ENZYME_INFO _must_ be at the end of this parameters file
#
[COMET_ENZYME_INFO]
0.  No_enzyme              0      -           -
1.  Trypsin                1      KR          P
2.  Trypsin/P              1      KR          -
3.  Lys_C                  1      K           P
4.  Lys_N                  0      K           -
5.  Arg_C                  1      R           P
6.  Asp_N                  0      D           -
7.  CNBr                   1      M           -
8.  Glu_C                  1      DE          P
9.  PepsinA                1      FL          P
10. Chymotrypsin           1      FWYL        P
#+end_src


#+begin_src nextflow

process MakeCometConfig {
    // should we instead return a tuple here of fastadb and config
    // because the config.txt refers to it?
    input:
    file fastadb_with_decoy from joined_fasta_with_decoys_ch.first()
    path template from Channel.fromPath(params.comet_template)
    output:
    file "comet_config.txt" into comet_config_ch
    """
    sed 's/@DDA_DB_FILE@/$fastadb_with_decoy/g;s/@FRAGMENT_MASS_TOLERANCE@/$params.fragment_mass_tolerance/g;s/@PRECURSOR_MASS_TOLERANCE@/$params.precursor_mass_tolerance/g' $template > comet_config.txt 
    """
    
}
#+end_src


setting memory & error strategy like this prevents caching
even with process.cache='lenient'
maybe because the task.attempt = 1
is tried first 
#+begin_src nextflow
process Comet {
    // we probably also want to publish thees
    memory { 5.GB * 2 *  task.attempt }
    errorStrategy { task.exitStatus in 137..137 ? 'retry' : 'terminate' }
    maxRetries 2
    input:
    file comet_config from comet_config_ch.first()
    // future dev: we can .mix with DDA here?
    // though we might need to tag for DDA / Pseudo
    // so that xinteract 
    file mzxml from maybespectra_ch.comet
    file fastadb_with_decoy from joined_fasta_with_decoys_ch.first()
    output:
    file("${mzxml.baseName}.pep.xml") into comet_pepxml_ch
    file mzxml into xinteract_comet_mzxml_ch
    when:
    params.search_engines.contains("comet")

    """
    /opt/comet/comet-ms -P$comet_config $mzxml 
    """
}
#+end_src

#+begin_src nextflow
process XinteractComet {
    memory '16 GB'
    time '5h'
    // memory usage scales with the number of input files
    // find the correct usage per input file or size
    // also for xinteractxtandem
    // usage there seems to be a lot smaller
    // as input files seems to be smaller
    input:
    file pepxmls from comet_pepxml_ch.toSortedList()
    // the filename of needed fastdadb was defined in cometcfg
    // and stored in pepxml in the comet-ms step
    // -a suppplies the absulute path to the data directory where the mzxmls
    // rather than reading wherer the mfrom the xmls
    // where the mzxml are, because its not very
    // nextflow to look outside the cwd.
    file fastadb_with_decoy from joined_fasta_with_decoys_ch.first()
    file mzxmls from  xinteract_comet_mzxml_ch.toSortedList()
    output: 
    file "interact_comet.pep.xml" into comet_search_results_ch 
    """
    /opt/tpp/bin/xinteract -a\$PWD -OARPd -dDECOY_ -Ninteract_comet.pep.xml $pepxmls
    """
}
#+end_src

Some scientific papers use =mmu= which is equal to 1 milidalton
0.001 Dalton
#+begin_src nextflow :noweb-ref nf-params :tangle no
params.precursor_mass_tolerance=10 // ppm 
params.fragment_mass_tolerance=0.02  // Dalton
params.comet_template="comet_template.txt"
#+end_src

Notably, a more stringent (lower) tolerance increases memorary usage by comet.

*** DONE Xtandem
:LOGBOOK:

CLOCK: [2022-06-22 Wed 10:24]--[2022-06-22 Wed 16:28] =>  6:04
CLOCK: [2022-06-21 Tue 10:15]--[2022-06-21 Tue 19:59] =>  9:44
CLOCK: [2022-06-09 Thu 09:31]--[2022-06-09 Thu 18:58] =>  9:27
- State "TODO"       from              [2022-06-06 Mon 09:56]
:END:
[[file:glaDIAtor/UI/ui/__init__.py::workflow.runXTandem]]
[[file:glaDIAtor/workflow.py::def runXTandem]]

#+NAME: taxonomy-template
#+begin_src xml 
<?xml version="1.0"?>
<bioml label="x! taxon-to-file matching list">
  <taxon label="DB">
    <file format="peptide" URL="%s" />
  </taxon>
</bioml>
#+end_src

#+NAME: xtandem-configuration-template
#+begin_src xml :tangle xtandem-template.xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="tandem-input-style.xsl"?>
<bioml>
<note>list path parameters</note>

<note>spectrum parameters</note>
	<note type="input" label="spectrum, fragment monoisotopic mass error">@FRAGMENT_MASS_TOLERANCE@</note>
	<note type="input" label="spectrum, parent monoisotopic mass error plus">@PRECURSOR_MASS_TOLERANCE@</note>
	<note type="input" label="spectrum, parent monoisotopic mass error minus">@PRECURSOR_MASS_TOLERANCE@</note>
	<note type="input" label="spectrum, parent monoisotopic mass isotope error">yes</note>
	<note type="input" label="spectrum, fragment monoisotopic mass error units">Daltons</note>
	<note>The value for this parameter may be 'Daltons' or 'ppm': all other values are ignored</note>
	<note type="input" label="spectrum, parent monoisotopic mass error units">ppm</note>
		<note>The value for this parameter may be 'Daltons' or 'ppm': all other values are ignored</note>
	<note type="input" label="spectrum, fragment mass type">monoisotopic</note>
		<note>values are monoisotopic|average </note>

<note>spectrum conditioning parameters</note>
	<note type="input" label="spectrum, dynamic range">100.0</note>
		<note>The peaks read in are normalized so that the most intense peak
		is set to the dynamic range value. All peaks with values of less that
		1, using this normalization, are not used. This normalization has the
		overall effect of setting a threshold value for peak intensities.</note>
	<note type="input" label="spectrum, total peaks">50</note> 
		<note>If this value is 0, it is ignored. If it is greater than zero (lets say 50),
		then the number of peaks in the spectrum with be limited to the 50 most intense
		peaks in the spectrum. X! tandem does not do any peak finding: it only
		limits the peaks used by this parameter, and the dynamic range parameter.</note>
	<note type="input" label="spectrum, maximum parent charge">4</note>
	<note type="input" label="spectrum, use noise suppression">yes</note>
	<note type="input" label="spectrum, minimum parent m+h">500.0</note>
	<note type="input" label="spectrum, minimum fragment mz">150.0</note>
	<note type="input" label="spectrum, minimum peaks">15</note> 
	<note type="input" label="spectrum, threads">40</note>
	<note type="input" label="spectrum, sequence batch size">1000</note>
	
<note>residue modification parameters</note>
	<note type="input" label="residue, modification mass">57.022@C</note>
		<note>The format of this parameter is m@X, where m is the modfication
		mass in Daltons and X is the appropriate residue to modify. Lists of
		modifications are separated by commas. For example, to modify M and C
		with the addition of 16.0 Daltons, the parameter line would be
		+16.0@M,+16.0@C
		Positive and negative values are allowed.
		</note>
	<note type="input" label="residue, potential modification mass">16@M</note>
		<note>The format of this parameter is the same as the format
		for residue, modification mass (see above).</note>
	<note type="input" label="residue, potential modification motif"></note>
		<note>The format of this parameter is similar to residue, modification mass,
		with the addition of a modified PROSITE notation sequence motif specification.
		For example, a value of 80@[ST!]PX[KR] indicates a modification
		of either S or T when followed by P, and residue and the a K or an R.
		A value of 204@N!{P}[ST]{P} indicates a modification of N by 204, if it
		is NOT followed by a P, then either an S or a T, NOT followed by a P.
		Positive and negative values are allowed.
		</note>

<note>protein parameters</note>
	<note type="input" label="protein, taxon">other mammals</note>
		<note>This value is interpreted using the information in taxonomy.xml.</note>
	<note type="input" label="protein, cleavage site">[RK]|{P}</note>
		<note>this setting corresponds to the enzyme trypsin. The first characters
		in brackets represent residues N-terminal to the bond - the '|' pipe -
		and the second set of characters represent residues C-terminal to the
		bond. The characters must be in square brackets (denoting that only
		these residues are allowed for a cleavage) or french brackets (denoting
		that these residues cannot be in that position). Use UPPERCASE characters.
		To denote cleavage at any residue, use [X]|[X] and reset the 
		scoring, maximum missed cleavage site parameter (see below) to something like 50.
		</note>
	<note type="input" label="protein, modified residue mass file"></note>
	<note type="input" label="protein, cleavage C-terminal mass change">+17.002735</note>
	<note type="input" label="protein, cleavage N-terminal mass change">+1.007825</note>
	<note type="input" label="protein, N-terminal residue modification mass">0.0</note>
	<note type="input" label="protein, C-terminal residue modification mass">0.0</note>
	<note type="input" label="protein, homolog management">no</note>
	<note>if yes, an upper limit is set on the number of homologues kept for a particular spectrum</note>
	<note type="input" label="protein, quick acetyl">no</note>
	<note type="input" label="protein, quick pyrolidone">no</note>

<note>model refinement parameters</note>
	<note type="input" label="refine">yes</note>
	<note type="input" label="refine, modification mass"></note>
	<note type="input" label="refine, sequence path"></note>
	<note type="input" label="refine, tic percent">20</note>
	<note type="input" label="refine, spectrum synthesis">yes</note>
	<note type="input" label="refine, maximum valid expectation value">0.1</note>
	<note type="input" label="refine, potential N-terminus modifications">+42.010565@[</note>


	<note type="input" label="refine, potential C-terminus modifications"></note>
	<note type="input" label="refine, unanticipated cleavage">yes</note>
	<note type="input" label="refine, potential modification mass"></note>
	<note type="input" label="refine, point mutations">no</note>
	<note type="input" label="refine, use potential modifications for full refinement">no</note>
	<note type="input" label="refine, point mutations">no</note>
	<note type="input" label="refine, potential modification motif"></note>
	<note>The format of this parameter is similar to residue, modification mass,
		with the addition of a modified PROSITE notation sequence motif specification.
		For example, a value of 80@[ST!]PX[KR] indicates a modification
		of either S or T when followed by P, and residue and the a K or an R.
		A value of 204@N!{P}[ST]{P} indicates a modification of N by 204, if it
		is NOT followed by a P, then either an S or a T, NOT followed by a P.
		Positive and negative values are allowed.
		</note>

<note>scoring parameters</note>
	<note type="input" label="scoring, minimum ion count">4</note>
	<note type="input" label="scoring, maximum missed cleavage sites">1</note>
	<note type="input" label="scoring, x ions">no</note>
	<note type="input" label="scoring, y ions">yes</note>
	<note type="input" label="scoring, z ions">no</note>
	<note type="input" label="scoring, a ions">no</note>
	<note type="input" label="scoring, b ions">yes</note>
	<note type="input" label="scoring, c ions">no</note>
	<note type="input" label="scoring, cyclic permutation">no</note>
		<note>if yes, cyclic peptide sequence permutation is used to pad the scoring histograms</note>
	<note type="input" label="scoring, include reverse">no</note>
		<note>if yes, then reversed sequences are searched at the same time as forward sequences</note>
	<note type="input" label="scoring, cyclic permutation">no</note>
	<note type="input" label="scoring, include reverse">no</note>

<note>output parameters</note>
	<note type="input" label="output, log path"></note>
	<note type="input" label="output, message">testing 1 2 3</note>
	<note type="input" label="output, one sequence copy">no</note>
	<note type="input" label="output, sequence path"></note>
	<note type="input" label="output, path">output.xml</note>
	<note type="input" label="output, sort results by">protein</note>
		<note>values = protein|spectrum (spectrum is the default)</note>
	<note type="input" label="output, path hashing">no</note>
		<note>values = yes|no</note>
	<note type="input" label="output, xsl path">tandem-style.xsl</note>
	<note type="input" label="output, parameters">yes</note>
		<note>values = yes|no</note>
	<note type="input" label="output, performance">yes</note>
		<note>values = yes|no</note>
	<note type="input" label="output, spectra">yes</note>
		<note>values = yes|no</note>
	<note type="input" label="output, histograms">yes</note>
		<note>values = yes|no</note>
	<note type="input" label="output, proteins">yes</note>
		<note>values = yes|no</note>
	<note type="input" label="output, sequences">yes</note>
		<note>values = yes|no</note>
	<note type="input" label="output, one sequence copy">no</note>
		<note>values = yes|no, set to yes to produce only one copy of each protein sequence in the output xml</note>
	<note type="input" label="output, results">valid</note>
		<note>values = all|valid|stochastic</note>
	<note type="input" label="output, maximum valid expectation value">0.1</note>
		<note>value is used in the valid|stochastic setting of output, results</note>
	<note type="input" label="output, histogram column width">30</note>
		<note>values any integer greater than 0. Setting this to '1' makes cutting and pasting histograms
		into spread sheet programs easier.</note>
<note type="description">ADDITIONAL EXPLANATIONS</note>
	<note type="description">Each one of the parameters for X! tandem is entered as a labeled note
			node. In the current version of X!, keep those note nodes
			on a single line.
	</note>
	<note type="description">The presence of the type 'input' is necessary if a note is to be considered
			an input parameter.
	</note>
	<note type="description">Any of the parameters that are paths to files may require alteration for a 
			particular installation. Full path names usually cause the least trouble,
			but there is no reason not to use relative path names, if that is the
			most convenient.
	</note>
	<note type="description">Any parameter values set in the 'list path, default parameters' file are
			reset by entries in the normal input file, if they are present. Otherwise,
			the default set is used.
	</note>
	<note type="description">The 'list path, taxonomy information' file must exist.
		</note>
	<note type="description">The directory containing the 'output, path' file must exist: it will not be created.
		</note>
	<note type="description">The 'output, xsl path' is optional: it is only of use if a good XSLT style sheet exists.
		</note> 
</bioml>
#+end_src

# desire to configure this with guile intensifies
#+NAME: xtandem-input-template
#+begin_src xml
<?xml version="1.0"?>
<bioml>
	<note>
	Each one of the parameters for x! tandem is entered as a labeled note node. 
	Any of the entries in the default_input.xml file can be over-ridden by
	adding a corresponding entry to this file. This file represents a minimum
	input file, with only entries for the default settings, the output file
	and the input spectra file name. 
	See the taxonomy.xml file for a description of how FASTA sequence list 
	files are linked to a taxon name.
	</note>

	<note type="input" label="list path, default parameters">%s</note>
	<note type="input" label="list path, taxonomy information">%s</note>

	<note type="input" label="protein, taxon">DB</note>
	
	<note type="input" label="spectrum, path">%s</note>

	<note type="input" label="output, path">%s</note>
</bioml>
#+end_src


We are making the =xtandem_taxonomy= xml in the same process because its kinda a pseudo dependency 

#+begin_src nextflow :noweb no-export
process MakeXtandemConfig {
    input:
    file template from Channel.fromPath(params.xtandem_template)
    file fastadb_with_decoy from joined_fasta_with_decoys_ch.first()
    output:
    file "xtandem_config.xml" into xtandem_config_ch
    """
    sed 's/@DDA_DB_FILE@/$fastadb_with_decoy/g;s/@FRAGMENT_MASS_TOLERANCE@/$params.fragment_mass_tolerance/g;s/@PRECURSOR_MASS_TOLERANCE@/$params.precursor_mass_tolerance/g' $template > xtandem_config.xml
    """
}


process XTandem {
    when:
    params.search_engines.contains("xtandem")

    input:
    file mzxml from maybespectra_ch.xtandem
    file tandem_config from xtandem_config_ch.first()
    file fastadb_with_decoy from joined_fasta_with_decoys_ch.first()
    output:
    file("${mzxml.baseName}.tandem.pep.xml") into xtandem_pepxml_ch
    file mzxml into xinteract_xtandem_mzxml_ch
    """
    printf '
    <<taxonomy-template>>'  $fastadb_with_decoy | tail -n+2 > xtandem_taxonomy.xml
    
    printf '
    <<xtandem-input-template>>' $tandem_config xtandem_taxonomy.xml $mzxml ${mzxml.baseName}.TANDEM.OUTPUT.xml | tail -n+2 > input.xml
    /opt/tandem/tandem input.xml
    /opt/tpp/bin/Tandem2XML ${mzxml.baseName}.TANDEM.OUTPUT.xml ${mzxml.baseName}.tandem.pep.xml 
    """
}

process XinteractXTandem {
    memory '16 GB'
    input:
    file pepxmls from xtandem_pepxml_ch.toSortedList()
    // the filename of needed fastdadb was defined in cometcfg
    // and stored in pepxml in the comet-ms step
    // -a suppplies the absulute path to the data directory where the mzxmls
    // rather than reading wherer the mfrom the xmls
    // where the mzxml are, because its not very
    // nextflow to look outside the cwd.
    file fastadb_with_decoy from joined_fasta_with_decoys_ch.first()
    file mzxmls from  xinteract_xtandem_mzxml_ch.toSortedList()
    output: 
    file "interact_xtandem.pep.xml" into xtandem_search_results_ch 
    """
    /opt/tpp/bin/xinteract -a\$PWD -OARPd -dDECOY_ -Ninteract_xtandem.pep.xml $pepxmls
    """
}

#+end_src

#+begin_src nextflow :noweb-ref nf-params :tangle no
params.xtandem_template="xtandem-template.xml"
#+end_src
| # pepxml | size pepxml (GiB) |
|       69 |         0.7890625 |
|          |                   |
#+TBLFM: @2$2=808/1024
Why is this much smaller than comet?


We patch tpp to accept comet 2019015 

#+begin_src diff :tangle tpp-5.2-fix.diff
diff -Naur release_5-2-0/extern/Makefile release_5-2-0_mod/extern/Makefile
--- release_5-2-0/extern/Makefile	2020-07-29 23:43:45.483620066 +0300
+++ release_5-2-0_mod/extern/Makefile	2020-07-29 23:47:41.796860274 +0300
@@ -339,7 +339,7 @@
 #
 # http://comet-ms.sourceforge.net/
 #
-COMET_VER := 2018014
+COMET_VER := 2019015
 COMET_ZIP := $(TPP_EXT)/comet_source_$(COMET_VER).zip
 COMET_SRC := $(BUILD_SRC)/comet_source_$(COMET_VER)
 .PHONY: comet comet-source comet-clean
#+end_src

*** DONE Joining Comet & Xtandem
[[file:glaDIAtor/workflow.py::combine_search_engine_results(event,]]
[[file:glaDIAtor/workflow.py::def combine_search_engine_results]]
#+begin_src nextflow

#+end_src

the tap seems to hap after nextflow has stopped,
look more into this.

possible causes:
[[https://github.com/nextflow-io/nextflow/issues/2502][Queue remains open when data is staged from an external source  Issue #2502  nextflow-io/nextflow  GitHub]]
[[https://github.com/nextflow-io/nextflow/issues/1230][Parent nextflow process doesn't exit after all compute tasks are complete  Issue #1230  nextflow-io/nextflow  GitHub]]


#+begin_src nextflow
// we handle the one or two engines case
// DSL2 incompat
// would be in workflow body

if (params.search_engines.size() > 1) {  
    process CombineSearchResults {
	publishDir "${params.outdir}/speclib"
	when:
	
	input:
	file xtandem_search_results from xtandem_search_results_ch
	file comet_search_results from comet_search_results_ch
	output:
	file "lib_iprophet.peps.xml" into combined_search_results_ch
	"""
	/opt/tpp/bin/InterProphetParser DECOY=DECOY_ THREADS=${task.cpus} $xtandem_search_results $comet_search_results lib_iprophet.peps.xml
	"""
    }
} else if (params.search_engines.contains("comet")) {
    combined_search_results_ch = comet_search_results_ch
} else if (params.search_engines.contains("xtandem")) {
    combined_search_results_ch =xtandem_search_results_ch
} else {
    combined_search_results_ch = Channel.create()
}
#+end_src
*** DONE Building Specral Library
[[file:glaDIAtor/UI/ui/__init__.py::workflow.buildlib(]]
[[file:glaDIAtor/workflow.py::def buildlib(event, \\]]
Inputs from [[*Creating Swath window files][Creating Swath window files]]
http://www.openswath.org/en/latest/docs/openswath.html

**** Mayu
doi:10.5167/uzh-28712
doi:10.1074/mcp.M900317-MCP200
#+begin_quote
	GENERAL:
	Mayu is a software package to determine protein
	identification false discovery rates (protFDR) and
	peptide identification false discovery rates (pepFDR)
	additionally to the peptide-spectrum match false discovery 
	rate (mFDR).
#+end_quote

Here is what happens in mayu:
For a pepxml file with peptide-spectrum-matches =PSM=
(type of =(spectrum,peptide,probability)=, where the probality is based on the similarity of the theoratical spectrum,
mayu determines the peptide-spectrum-match False Detection Rate (=mFDR=),
and protein identification false discovery rates (=protFDR=).
We select a =protFDR= for mayu finds a matching =mFDR= level (no higher than the =-G= flag) and it will filter
everything with a higher mFDR level
In the output csv the =score= column is the the =probability= in PSM (in mayu documentation "discrimant")

We find the lowest =probability= that still has an =mFDR= that matched the above,
and that is what we use as the filtering criterian in spectrast

This is what we will than filter on with specrtrast

Hmhf why can't mayu return deterministic filenames.
(It incorporates the mayu version number in the filename grumbl),
it follows the pattern
#+begin_src perl :eval no :exports code :tangle no
my $psm_file_base = $out_base . '_psm_';
my $id_csv_file = $psm_file_base
                . $fdr_type
                . $fdr_value . '_'
                . $target_decoy . '_'
                . $version . '.csv';
                
#+end_src

#+begin_src nextflow 
process  FindMinimumPeptideProbability {
    input: 
    file combined_search_results from combined_search_results_ch.first()
    file fastadb_with_decoy from joined_fasta_with_decoys_ch.first()
    output:
    env PEPTIDEPROBABILITY into  minimum_peptide_probability
    /* explanation of paramaters
     -G  $params.protFDR            | maximum allowed mFDR of $params.protFDR 
     -P protFDR=$params.protFDR:t   |  print out PSMs of targets who have a protFDR of $params.protFDR
     -
     -H | defines the resolution of error analysis (mFDR steps)
     -I number of missed cleavages used for database search
     -M | file name base
     ,*/
    script:
    prefix="filtered"
    // you can change this to a glob-pattern (e.g. "*") for future-proofing
    mayu_version="1.07"
    psm_csv="${prefix}_psm_protFDR${params.protFDR}_t_${mayu_version}.csv"
    """
    /opt/tpp/bin/Mayu.pl -verbose -A $combined_search_results -C $fastadb_with_decoy -E DECOY_ -G $params.protFDR -P protFDR=${params.protFDR}:t -H 51 -I 2 -M $prefix
    # test if psm_csv was made
    test -e $psm_csv || exit 1
    # test if the results arent empty
    test `wc -l $psm_csv | cut -d' ' -f1` -gt 1 || exit 1 
    PEPTIDEPROBABILITY=`cat $psm_csv | cut -f 5 -d ',' |tail -n+2 |sort -u | head -n1`
    """
}
#+end_src

Note that sort requires =$TMPDIR= to actually exists and be writable,
=$TMPDIR= (the envvar) is inherited from the parent env when run in a container,
but not mounted (at least not in Singularity), so if =$TMPDIR= does not exist in the container, this will crash.

#+begin_src  nextflow :noweb-ref nf-params :tangle no
// sensible values = floats between 0 and 1 
// target FDR for mayu
// this is equivalent to the "pvalue" parameter in the original (python) gladiator implementation
// which is labeld as "Spectral library building FDR" in the UI
params.protFDR=0.01
#+end_src

**** Spectrast
http://tools.proteomecenter.org/wiki/index.php?title=Software:SpectraST

Spectrast in =SpectraSTPepXMLLibImporter.cpp= =readFromFile= =processSearchHit= will read
the mzxmls contained in the pepxml. It defaults to looking for the mzxml in the CWD
otherwise it checks the path in the =base_name= property of msms_run_summary element in =<search_summary=
so we need again give the =maybespectra_ch= on.
From the above url
#+begin_quote
- Creating Consensus Libraries
1. Importing the raw spectra into SpectraST
[ ... ]
Remember that the .mzXML files must be in the same directories as their
corresponding .pepXML files. 
#+end_quote

Spectrast is from =tpp=
Note that spectrast flags are single-dash multilettered underscored argument-concatenated.thanks.
Its argument-parser is *very* funky, so be careful here.
It also doesn't check if illegal flags are given, they will pass silently instead, *grumble.*
#+begin_src fundamental :tangle no
(\_/) .~~ 
(._.)/~~~
(_ _)     
#+end_src

***** Converting traml into spectrast friendly format

#+NAME: irttsv-to-spectrasttsv
#+begin_src awk :eval no
BEGIN {FS="	"; OFS="	"}
NR==1 {
    for (i=1; i<=NF; i++) {
        f[$i] = i
    }
}
NR>1 { print $(f["PeptideSequence"]), $(f["NormalizedRetentionTime"]) }
#+end_src

#+NAME: irttsv-to-spectrasttsv-without-duplicates
#+begin_src awk :eval no
BEGIN {FS="	"; OFS="	"}
# we set the column names so that we can look them up later
NR==1 {
    for (i=1; i<=NF; i++) {
        f[$i] = i
    }
}
# use only the last entry in the table per peptide sequence
NR>1 {
    irt_by_sequence[$(f["PeptideSequence"])] = $(f["NormalizedRetentionTime"])
    peptide_sequences[$(f["PeptideSequence"])]=$(f["PeptideSequence"])
}
END {
    for (sequence in peptide_sequences)
	print(sequence,irt_by_sequence[sequence])
}
#+end_src

TargetedFileConverter from =OpenMS=

#+begin_src nextflow :noweb yes
process CreateSpectrastIrtFile {
    input:
    file irt_traml from Channel.fromPath(params.irt_traml_file)
    output:
    file ("irt.txt") into irt_txt_ch
    script:
    intermediate_tsv="intermediate_irt.tsv"
    """
    TargetedFileConverter -in $irt_traml -out_type tsv -out $intermediate_tsv
    """ + '''  awk '
    <<irttsv-to-spectrasttsv>>' ''' + "$intermediate_tsv > irt.txt"
}
#+end_src
***** Running Spectrast
#+begin_src nextflow
// spectrast will create *.splib, *.spidx, *.pepidx, 
// note that where-ever a splib goes, so must its spidx and pepidx
///and they must have the same part
process SpectrastCreateSpecLib {
    input:
    file irtfile from irt_txt_ch
    file combined_search_results from combined_search_results_ch.first()
    file fastadb_with_decoy from joined_fasta_with_decoys_ch.first()
    file spectra from maybespectra_ch.spectrast.toSortedList()
    val cutoff from minimum_peptide_probability
    output:
    tuple file ("${prefix}_cons.splib"), file("${prefix}_cons.spidx") into spectrast_ch
    file("${prefix}_cons.sptxt") into consensus_lib_sptxt_ch
    script:
    prefix = "SpecLib"
    to_run = "/opt/tpp/bin/spectrast -cN${prefix} -cIHCD -cf\"Protein! ~ DECOY_\" -cP$cutoff -c_IRR "
    if (params.use_irt)
	to_run += "-c_IRT$irtfile "
    to_run +=  "$combined_search_results" // spectrast really wants its input-files last.
    to_run += "\n /opt/tpp/bin/spectrast -cN${prefix}_cons -cD$fastadb_with_decoy -cIHCD -cAC ${prefix}.splib"
}
 #+end_src

from original gladiator implementation
source is unclear;
author forgot.

#+begin_src fundamental :tangle irt.txt 
LGGNEQVTR   -28.308
GAGSSEPVTGLDAK  0.227
VEATFGVDESNAK   13.1078
YILAGVENSK  22.3798
TPVISGGPYEYR    28.9999
TPVITGAPYEYR    33.6311
DGLDAASYYAPVR   43.2819
ADVTPADFSEWSK   54.969
GTFIIDPGGVIR    71.3819
GTFIIDPAAVIR    86.7152
LFLQFGAQGSPFLK  98.0897
#+end_src

#+begin_src nextflow :noweb-ref nf-params :tangle no
// white-space-delimited file of peptide-sequences and internal retention times
// whether or not to use the retention-
params.use_irt=true
params.irt_traml_file = "iRTAssayLibrary.TraML"
 #+end_src
**** OpenSwathDecoys
specrast2tsv.py is from =msproteomicstools=
OpenSwathDecoyGenerator from =OpenMS= =topp=
#+begin_src nextflow :noweb-ref nf-params :tangle no
// optional 
params.openswath_transitions = ""

#+end_src

#+begin_src nextflow
process CreateOpenswathTransitions {
    /*
     Choice parts of sprectrast2.tsv --help
     
     spectrast2tsv.py
     ---------------
     This script is used as filter from spectraST files to swath input files.
     python spectrast2tsv.py [options] spectrast_file(s)
     
     -d                  Remove duplicate masses from labeling
     -e                  Use theoretical mass
     -k    output_key    Select the output provided. Keys available: openswath, peakview. Default: peakview
     -l    mass_limits   Lower and upper mass limits of fragment ions. Example: -l 400,2000
     -s    ion_series    List of ion series to be used. Example: -s y,b

     -w    swaths_file   File containing the swath ranges. This is used to remove transitions with Q3 falling in the swath mass range. (line breaks in windows/unix format)
     -n    int           Max number of reported ions per peptide/z. Default: 20
     -o    int           Min number of reported ions per peptide/z. Default: 3
     -a    outfile       Output file name (default: appends _peakview.txt)
     ,*/
    input:
    file swath_windows from swath_windows_ch
    file sptxt from consensus_lib_sptxt_ch
    output:
    file outputfile into openswath_transitions_ch

    script:
    outputfile="SpecLib_cons_decoys.pqp"
    
    """
    MINWINDOW=`head -n1 $swath_windows | cut -d'	' -f1`
    MAXWINDOW=`tail -n1 $swath_windows | cut -d'	' -f2`
    spectrast2tsv.py -l \$MINWINDOW,\$MAXWINDOW -s y,b -d -e -o 6 -n 6 -w $swath_windows -k openswath -a SpecLib_cons_openswath.tsv $sptxt
    TargetedFileConverter -in SpecLib_cons_openswath.tsv -out SpecLib_cons.TraML
    OpenSwathDecoyGenerator -decoy_tag DECOY_ -in SpecLib_cons.TraML -out $outputfile -method reverse
    """
}
#+end_src


=TargetedFileConverter= from msproteomics.
#+begin_quote
/usr/bin/TargetedFileConverter: error while loading shared libraries: libQt5Core.so.5: cannot open shared object file: No such file or directory
#+end_quote

Here we might actually not need TargetedFileConverter,
can give tsv directly to OpenSwathDecoyGenerator.
and pass result tsv to OpenSwathWorkflow as =-tr=.

*** OpenSwathDecoyGenerator


#+begin_src nextflow

// process OpenSwathDecoys{
//     "e
// }
#+end_src




** DONE Building Dia Matrix
https://openswath.org/_/downloads/en/latest/pdf/
https://openswath.org/_/downloads/en/latest/htmlzip/
*** OpenSwathWorkflow
Creates tsv with =-out_tsv=
[[file:glaDIAtor/workflow.py::def buildDIAMatrix(\\]]

Using a the cache will decrease memomry usage at the cost of writes & time
#+begin_src nextflow :noweb-ref nf-params :tangle no
// wheter to use -readOptions cacheWorkingInMemory in OSW
// this actually crashes so disabled
params.osw_use_cache = false
// extra flags to pass to OSW
params.osw_extra_flags =  ""
#+end_src

to use ~-out_osw~, ~-tr~ needs to be a pqp file,


If we are using legacy pyprophet we will need to create a tsv
#+NAME: nf-openswathworkflow-for-pyprophet-legacy
#+begin_src nextflow :tangle no 
openswath_transitions_ch_for_legacy = openswath_transitions_ch
process OpenSwathWorkflow_legacy {
	memory '16 GB'
	input:
	file dia_mzml_file from dia_mzml_files_ch.osw
	// file openswath_transitions from Channel.fromPath("data_from_original/bruderer-pwiz-no-dda/SpecLib_cons_decoy.TraML").first()
	file openswath_transitions from openswath_transitions_ch.first()
	file swath_truncated_windows from truncated_swath_windows_ch.first()
	file irt_traml from Channel.fromPath(params.irt_traml_file).first()
	output:
	file dia_tsv_file  into openswath_tsv_ch
	script:
	dia_tsv_file = "${dia_mzml_file.baseName}-DIA.tsv"
	to_execute =
            "OpenSwathWorkflow " +
            "-force " +
            "-in $dia_mzml_file " +
            "-tr $openswath_transitions " +
            "-threads ${task.cpus} " +
            "-min_upper_edge_dist 1 " +
            "-sort_swath_maps " +
            "-out_tsv ${dia_tsv_file} " + 
            "-swath_windows_file $swath_truncated_windows " +
            params.osw_extra_flags + " "
	if (params.use_irt)
            to_execute +=  "-tr_irt $irt_traml "
	to_execute}
}
#+end_src


If we are using nonlegacy pyprophet we will need to create an osw

#+NAME: nf-openswathworkflow-for-pyprophet-nonlegacy
#+begin_src nextflow :tangle no
openswath_osw_indirect_ch = Channel.create()
openswath_osw_indirect_ch.multiMap{ it ->
    pyprophet_subsample: pyprophet_score  : it}.set{openswath_osw_ch}
openswath_transitions_ch_for_nonlegacy = openswath_transitions_ch
process OpenSwathWorkflow {
    memory '16 GB'
    input:
    file dia_mzml_file from dia_mzml_files_ch.osw
    // file openswath_transitions from Channel.fromPath("data_from_original/bruderer-pwiz-no-dda/SpecLib_cons_decoy.TraML").first()
    file openswath_transitions from openswath_transitions_ch_for_nonlegacy.first()
    file swath_truncated_windows from truncated_swath_windows_ch.first()
    file irt_traml from Channel.fromPath(params.irt_traml_file).first()
    output:
    file dia_osw_file  into openswath_osw_indirect_ch
    script:
    dia_osw_file = "${dia_mzml_file.baseName}-DIA.osw"
    to_execute =
        "OpenSwathWorkflow " +
        "-force " +
        "-in $dia_mzml_file " +
        "-tr $openswath_transitions " +
        "-threads ${task.cpus} " +
        "-min_upper_edge_dist 1 " +
        "-sort_swath_maps " +
        "-out_osw ${dia_osw_file} " + 
        "-swath_windows_file $swath_truncated_windows " +
        params.osw_extra_flags
    
    if (params.use_irt)
        to_execute +=  "-tr_irt $irt_traml "
    to_execute}
}
#+end_src

Then here we choose which one to use
#+begin_src nextflow :noweb no-export
// we will need the osw to go to various processes
if (params.pyprophet_use_legacy){
 <<nf-openswathworkflow-for-pyprophet-legacy>>
} else {
  <<nf-openswathworkflow-for-pyprophet-nonlegacy>>
}
#+end_src
(Apparently these two cant have the same name, even if they are conditionally declared


#+begin_quote
Extraction windows have a gap. Will abort (override with -force)
#+end_quote

OpenSwathWorkflow invocation can output tsv XOR osw, not both.
You will get exit status 8 
#+begin_quote
  Error: Unexpected internal error (Either out_features, out_tsv or out_osw needs to be set (but not two or three at the same time))
#+end_quote

**** What is the relation between =irt.txt= and =iRTAssayLibrary.TraML=

The =irt.txt= seems to contain pairs of 
~CompoundList/Peptide@sequence~ and ~Compoundlist/Peptide/RetentionTimeList/cvParam[@name="normalized retention time"]@value~
of the traml file, except
#+begin_quote
LFLQFGAQGSPFLK  98.0897
#+end_quote
is not present in the traml file.
# xpath notation https://www.w3schools.com/xml/xpath_syntax.asp

traml was _not_ gotten from here:
https://db.systemsbiology.net/sbeams/cgi/PeptideAtlas/PASS_View?identifier=PASS00779
file:/ftp:PASS00779@ftp.peptideatlas.org:/
(but is comparable)

#+begin_quote
<OA> The files have been downloaded from net and for now they are
	      intended to be used "as is". If there becomes a need to modify
	      those, then there will be a need figure out how to do it. So, it
	      is very much unexplored how to generate those files for now.
								        [10:56]
<NA> looks like the .txt contains pairs of "sequence - Normalized retention
       time" from the traml. The only one in the .txt that isnt in the traml
       seems to be LFLQFGAQGSPFLK  [10:59]
<NA> where did you download them froM?  [11:00]
<OA> That is a good question. I guess it was some example dataset for
	      openswath, but I don't remember which. If you have time and
	      interest, you could try to figure out how iRT assay library
	      should be built.  [11:07]
#+end_quote

#+begin_quote

Retention time normalization

The retention time normalization peptides are provided using the optional parameter tr_irt in TraML format. We
suggest to use the iRTassays.TraML file provided in the tutorial dataset, if the Biognosys iRT-kit was used during sample
preparation.

If the iRT-kit was not used, it is highly recommended to use or generate a set of endogenous peptides for RT
normalization. A recent publication [5] provides such a set of CiRT peptides suitable for many eukaryotic samples. The
TraML file from the supplementary information can be used as input for tr_irt. Since not all CiRT peptides might be
found, the flag RTNormalization:estimateBestPeptides should be set to improve initial filtering of poor signals.
Further parameters for optimization can be found when invoking OpenSwathWorkflow --helphelp under the
RTNormalization section. Those do not require adjustment for most common sample types and LC-MS/MS setups, but
might be useful to tweak for specific scenarios.
#+end_quote
#+begin_quote
5 Rst HL, Liu Y, DAgostino G, Zanella M, Navarro P, Rosenberger G, Collins BC, Gillet L, Testa G, Malmstrm L, Aebersold R. TRIC:
an automated alignment strategy for reproducible protein quantification in targeted proteomics. Nat Methods. 2016 Sep;13(9):777-83. doi:
10.1038/nmeth.3954. Epub 2016 Aug 1. PMID: 27479329
#+end_quote
doi:10.1038/nmeth.3954
file:/ftp:PASS00788@ftp.peptideatlas.org:/

also not from here

It seems the traml is based on
file:/ftp:PASS00289@ftp.peptideatlas.org:/SGS/assays/OpenSWATH_SM4_iRT_AssayLibrary.TraML
which has also has the same irt times as iRT.txt,
except its still missign =LFLQFGAQGSPFLK=,
The same irt.txt can be found in https://github.com/CaronLab/Allele-specific-library-scripts/blob/main/iRT.txt,
published before gladiator, with the retention times
also hardcoded in https://github.com/msproteomicstools/msproteomicstools/raw/master/analysis/spectral_libs/spectrast_updateiRTs.py

 =OpenMs/src/tests/class_tests/openms/data/MRMDecoyGenerator_input.TraML= has the same irts and also contains
=LFLQFGAQGSPFLK= so that might also be a good target.


*** Pyprophet
https://github.com/PyProphet/pyprophet
=pyprophet=
License: 3-clause BSD
Pyprophet aggregates various quality scores into one score
=d_score= (discriminant_score)

**** Legacy Pyprophet
#+begin_src nextflow
if (params.pyprophet_use_legacy)
    process pyprophet_legacy {
    publishDir "${params.outdir}/pyprophet/", pattern: "*.csv"
    publishDir "${params.outdir}/reports/pyprophet/", pattern: "*.pdf"
    input:
    file dia_tsv from openswath_tsv_ch
    output:
    file dscore_csv into pyprophet_legacy_ch
    // just for publishing
    file "${dia_tsv.baseName}_report.pdf" 
    script:
    seed="928418756933579397"
    
    dscore_csv="${dia_tsv.baseName}_with_dscore.csv"
    """
    pyprophet --random_seed=${seed} --delim=tab --export.mayu ${dia_tsv} --ignore.invalid_score_columns
    """
}
#+end_src

#+begin_quote
WRITTEN:  B_D140314_SGSDSsample1_R01_MHRM_T0.mzML-DIA_summary_stat.csv
WRITTEN:  B_D140314_SGSDSsample1_R01_MHRM_T0.mzML-DIA_full_stat.csv
WRITTEN:  B_D140314_SGSDSsample1_R01_MHRM_T0.mzML-DIA_with_dscore.csv
WRITTEN:  B_D140314_SGSDSsample1_R01_MHRM_T0.mzML-DIA_with_dscore_filtered.csv
WRITTEN:  B_D140314_SGSDSsample1_R01_MHRM_T0.mzML-DIA_report.pdf
WRITTEN:  B_D140314_SGSDSsample1_R01_MHRM_T0.mzML-DIA_cutoffs.txt
WRITTEN:  B_D140314_SGSDSsample1_R01_MHRM_T0.mzML-DIA_svalues.txt
WRITTEN:  B_D140314_SGSDSsample1_R01_MHRM_T0.mzML-DIA_qvalues.txt
WRITTEN:  B_D140314_SGSDSsample1_R01_MHRM_T0.mzML-DIA_dscores_top_target_peaks.txt
WRITTEN:  B_D140314_SGSDSsample1_R01_MHRM_T0.mzML-DIA_dscores_top_decoy_peaks.txt
WRITTEN:  B_D140314_SGSDSsample1_R01_MHRM_T0.mzML-DIA_mayu.cutoff
WRITTEN:  B_D140314_SGSDSsample1_R01_MHRM_T0.mzML-DIA_mayu.fasta
WRITTEN:  B_D140314_SGSDSsample1_R01_MHRM_T0.mzML-DIA_mayu.csv
WRITTEN:  B_D140314_SGSDSsample1_R01_MHRM_T0.mzML-DIA_scorer.bin
WRITTEN:  B_D140314_SGSDSsample1_R01_MHRM_T0.mzML-DIA_weights.txt
#+end_quote

The pip pyprophet install would be

generated from =pip freeze= on dockerhub://elolab/gladiator,
and removing entries that are not in conda legacy-pyprophet

#+begin_src fundamental :tangle pyprophet-legacy-requirements.txt
backports.functools-lru-cache==1.6.4
cycler==0.10.0
kiwisolver==1.1.0
matplotlib==2.2.5
numexpr==2.7.3
numpy==1.16.5
pandas==0.24.2
pyparsing==2.4.7
# pyprophet==0.24.1
python-dateutil==2.8.1
pytz==2021.1
scikit-learn==0.20.4
scipy==1.2.3
seaborn==0.9.1
six==1.15.0
subprocess32==3.5.4
#+end_src

#+begin_src fundamental :tangle pyprophet-legacy-standalone.dockerfile
FROM ubuntu:20.04
MAINTAINER GlaDIAtorAdmin
ARG DEBIAN_FRONTEND=noninteractive
WORKDIR /tmp/
RUN apt-get update \
    && apt-get install -y apt-utils
# python-dev and g++ for compiling pyprophet
RUN apt-get update \
    && apt-get -y install wget python python-dev g++

RUN wget https://bootstrap.pypa.io/pip/2.7/get-pip.py -O get-pip.py
RUN python2 get-pip.py
COPY pyprophet-legacy-requirements.txt .
# we first install version locked requirements, then pyprophet in this order because pip is dumb.
RUN pip2 install -r pyprophet-legacy-requirements.txt
RUN pip2 install pyprophet==0.24.1
#+end_src

If you get an error here as
#+begin_quote
      raise Exception("got empty input file")
#+end_quote
try running with ~params.use_irt=false~
**** nonlegacy prypophet
http://www.openswath.org/en/latest/docs/pyprophet.html
we calculate the subsample ratio from the number of runs 
#+begin_src nextflow :noweb-ref nf-vars :tangle no
subsample_ratio = 1 / Math.max(Channel.fromPath(params.diafiles).toSortedList().length(), 1)
#+end_src


In =pyprophet subsample= and =pyprophet score=,
we will need to pass =--test= to not have random behaviour.
This will make the sql behaviour behaviour non-random.
However there is no actual seed paramater to set, its just a "use-random/ don't use random" flag.
#+begin_src nextflow :noweb-ref nf-params :tangle no
params.pyprophet_fixed_seed=true
params.pyprophet_use_legacy=false
#+end_src

Below we are following the steps of http://www.openswath.org/en/latest/docs/pyprophet.html#scaling-up

Anything in pyprophet that is not invoked with an ~--out~ flag
will overwrite the ~--in~ file,
here we only do that when the ~--in~-file is created in the process
#+begin_src nextflow 
if (!params.pyprophet_use_legacy)
    {
#+end_src

#+begin_src nextflow
process pyprophet_subsample {
    input:
    file dia_osw_file from openswath_osw_ch.pyprophet_subsample
    output:
    file subsampled_osw
    script:
    subsampled_osw="${dia_osw_file.baseName}.osws"
    pyprophet_seed_flag=(params.pyprophet_fixed_seed ? "--test" : "--no-test")
    """
    pyprophet subsample $pyprophet_seed_flag --in=$dia_osw_file --out=$subsampled_osw --subsample_ratio=$subsample_ratio
    """
}
#+end_src

#+begin_src nextflow
process pyprophet_learn_classifier {
    input:
    file subsampled_osws from subsampled_osw.toSortedList()
    file openswath_transitions from openswath_transitions_ch.first()
    output:
    file osw_model
    script:
    pyprophet_seed_flag=(params.pyprophet_fixed_seed ? "--test" : "--no-test")
    osw_model="model.osw"
    """
    pyprophet merge --template=$openswath_transitions $subsampled_osw --out=$osw_model $subsampled_osws
    pyprophet_score --score $pyprophet_seed_flag --in=$osw_model --level=ms1ms2
    """
}
#+end_src

#+begin_src nextflow
scored_osw_indirect_ch =Channel.create()
scored_osw_indirect_ch.multiMap{it ->
    reduce: backpropagate: it}.set{scored_osw_ch}

process pyprophet_apply_classifier {
    input:
    file osw_model from osw_model.first()
    file osw from openswath_osw_ch.pyprophet_score
    output:
    file scored_osw into scored_osw_indirect_ch
    script:
    pyprophet_seed_flag=(params.pyprophet_fixed_seed ? "--test" : "--no-test")
    scored_osw="${osw.baseName}.scored.${osw.Extension}"
    """
    pyprophet score $pyprophet_seed_flag --in=$osw --out=$scored_osw --apply_weights=$osw_model --level=ms1ms2
    """
}
#+end_src

#+begin_src nextflow
process pyprophet_reduce {
    input:
    file scored_osw from scored_osw_ch.reduce
    output:
    file reduced_scored_osw 
    script:
    reduced_scored_osw="${file(scored_osw.baseName).baseName}.${scored_osw.Extension}r"
    """
    pyprophet reduce --in=$scored_osw --out=$reduced_scored_osw
    """
}
#+end_src


#+begin_src nextflow
process pyprophet_control_error {
    input:
    file reduced_scored_osws from reduce_scored_osw.toSortedList()
    file osw_model from osw_model.first()
    output:
    file osw_global_model
    script:
    osw_global_model="model_global.osw"
    """
    pyprophet merge --template=$osw_model --out=$osw_global_model $reduced_scored_osws
    pyprophet peptide --context=global --in=$osw_model
    pyprophet protein --context=global --in=$osw_model
    """
}
#+end_src
#+begin_src nextflow
process pyprophet_backpropagate {
    input:
    file osw_scored from scored_osw_ch.backpropagate
    file osw_global_model from osw_global_model.first()
    output:
    file dscore_csv into pyprophet_nonlegacy_ch
    script:
    base="${file(osw_scored.baseName).baseName}"
    backproposw="${base}.backprop.osw"
    dscore_csv="${base}.csv"
    """
    pyprophet score --in="$osw" --apply_scores="$osw_global_model" --level=ms1ms2 --out=$backproposw
    pyprophet export --in=$backrproposwk --out=$dscore_csv
    """
}

#+end_src

#+begin_src nextflow
}
#+end_src

**** Choosing between legacy and nonlegacy pyprophet
Here we choose between legacy or nonlegacy pyprophet
#+begin_src nextflow
if (params.pyprophet_use_legacy)
    pyprophet_ch = pyprophet_legacy_ch
else
    pyprophet_ch = pyprophet_nonlegacy_ch
#+end_src

*** feature-alignment
of =msproteomicstools/analysis/alignment/feature_alignment.py=

Note: If things fail here,
because the right fdr cannot be reached,
you can try changing =precursor_mass_tolerance= and =fragment_mass_tolerance=
earlier upstream.

#+begin_src nextflow
process feature_alignment
{
    publishDir "${params.outdir}/dia/"
    input:
    file dscore_csvs from pyprophet_ch.toSortedList()
    output:
    file outfile into feature_alignment_ch
    script:
    outfile = "DIA-analysis-results.csv"
    
    if (params.use_irt) {
        realign_method = "diRT" 
    } else {
        realign_method = "linear"
    }
    
    "feature_alignment.py " +
        "--method best_overall " +
        "--max_rt_diff 90 " +
        "--target_fdr $params.trig_target_fdr " +
        "--max_fdr_quality $params.trig_max_fdr " +
        "--in $dscore_csvs " +         // will this break on filenames with spaces
        " --realign_method $realign_method " + 
        "--out $outfile"
}
#+end_src

#+begin_src nextflow :noweb-ref nf-params :tangle no
// Target FDR used in TRIG alignment in dirT mode [default 0.01]
// This was "trig_target_pvalue" in the original python gladiator implementation
params.trig_target_fdr=0.01
// Maximum FDR for TRIG alignment in dirT mode [default 0.05]
// This was "trig_max_pvalue" in the original python gladiator implementation
params.trig_max_fdr=0.05
#+end_src

*** Swath2stats

#+NAME: r-swath2stats
#+begin_src R :tangle no :eval no :var diafile="" :var strict_checking=0 :var peptideoutputfile="" :var proteinoutputfile="" :var decoyprefix="DECOY_"
.libPaths("/opt/Rlibs/") 

suppressPackageStartupMessages(library(SWATH2stats))
suppressPackageStartupMessages(library(data.table))

remove_irt <- function(df)
  df[grep("iRT", df[["ProteinName"]], invert=TRUE, fixed=TRUE),, drop=FALSE]

## original gladiator decoy removing behaviour
remove_decoy_strict <- function(df,decoyprefix)
  df[grep(decoyprefix, df[["ProteinName"]], invert=TRUE, fixed=TRUE),, drop=FALSE]


remove_decoy_loose <- function(df)
  df[!df[["decoy"]],, drop = FALSE]


basename_sans_ext <- function(f)
  unlist(strsplit(basename(f), ".",fixed=TRUE))[[1]]


main <- function(diafile,
                 strict_checking=FALSE,
                 peptideoutputfile="",
                 proteinoutputfile="",
                 decoyprefix="DECOY_")
{
  remove_decoy <- `if`(strict_checking,
                       function(df) remove_decoy_strict(df,decoyprefix),
                       remove_decoy_loose)
  filtered_data <-
    data.table::fread(diafile,header=TRUE) |> 
    data.frame(stringsAsFactors = FALSE) |>
    within(run_id <- basename(filename)) |>
    SWATH2stats::reduce_OpenSWATH_output() |>
    remove_irt() |>
    remove_decoy()

  # Writing output
  filtered_data |>
    SWATH2stats::write_matrix_peptides(filename = basename_sans_ext(peptideoutputfile)) |>
    write.table(sep="\t",file=peptideoutputfile,row.names = FALSE)

  filtered_data |>
    SWATH2stats::write_matrix_proteins(filename = basename_sans_ext(proteinoutputfile)) |>
    write.table(sep="\t",file=proteinoutputfile,row.names = FALSE)
}
#+end_src

Where the field "Decoy" is 1, thats a decoy generated by ~OpenSwathDecoyGenerator~,
rather than from the fastadataasee of ~DecoyDatabase~

#+begin_src nextflow :noweb no-export
process swath2stats {
    publishDir "${params.outdir}/dia/"
    input:
    file dia_score from feature_alignment_ch
    
    output:
    file peptide_matrix
    file protein_matrix
    
    script:
    strict_checking=params.swath2stats_strict_checking
    peptide_matrix="DIA-peptide-matrix.tsv"
    protein_matrix="DIA-protein-matrix.tsv"
    
    """
    #!/usr/bin/env Rscript
    """ +
        '''
<<r-swath2stats>>
        ''' +
        """
    main("$dia_score", strict_checking = as.logical("$strict_checking"),
	peptideoutputfile="$peptide_matrix",
	proteinoutputfile="$protein_matrix",
	decoyprefix="DECOY_")
	
    """
}
#+end_src

#+begin_src nextflow :noweb-ref nf-params :tangle no
// whether to exclude in final DIA matrices
// proteins of which
// any peptide can be a decoy.
// this is the default behaviour of original gladiator implementation.
// if set to false, just instead remove anything
// that has the "decoy" column set to false
params.swath2stats_strict_checking=true
#+end_src

**** Dependencies
#+NAME: gwl-swath2stats-deps
#+begin_src scheme :noweb-ref deps :tangle no
("swath2stats"
 "r-minimal"
 "r-swath2stats"
 "r-peca"
 "r-tidyr"
 "r-argparse"
 "r-corrplot")
#+end_src


The above as an R-script

#+begin_src R :eval no :tangle install-R-packages.R 
#!/usr/bin/env Rscript

.libPaths("/opt/Rlibs/")

if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install(version = "3.15",ask=FALSE,update=FALSE)

BiocManager::install("SWATH2stats", ask=FALSE,update=FALSE)
BiocManager::install("PECA", ask=FALSE,update=FALSE)
install.packages("tidyr",repos = "http://cran.us.r-project.org")
install.packages("argparse",repos = "http://cran.us.r-project.org")
install.packages("corrplot",repos = "http://cran.us.r-project.org")
#+end_src
* Configuration for backends
** Defining the config template
Here we define what the registry is that hosts our images
#+NAME: registry
: registry.gitlab.utu.fi/elixirdianf/gladiator-notes/

This is the template that we fill with the container manager program to be used (~%p~),
the gladiator container uri to be used (~%G~),
and the pyprophet legacy container to be used (~%P~).

#+NAME: container-config-template
#+begin_example nextflow
%p.enabled=true
process {
   container='%G'
    withName: pyprophet_legacy {
	container='%P'
    }
}
#+end_example

And here we write the logic that fills the template

#+NAME: construct-container-config
#+HEADER: :tangle no 
#+begin_src emacs-lisp :noweb yes :var program-name="singularity" :var locality="remote" :var template=container-config-template :var registry=registry
(defvar container-property-alist
  (let ((remote-registry registry))
    `(("remote"
       ("singularity"
	("access-protocol" . "docker://")
	("registry" . ,remote-registry)
	("container-suffix" . ""))
       ("docker"
	("access-protocol" . "https://")
	("registry" . ,remote-registry)
	("container-suffix" . "")))
      ("local"
       ("singularity"
	("access-protocol" . "file://")
	("registry" . "containers/")
	("container-suffix" . ".simg"))
       ("docker"
	("access-protocol" . "")
	("registry" . "localhost/")
	("container-suffix" . ""))))))

(defun construct-container-uri (basename program-name locality)
  (let ((properties
	 (alist-get
	  program-name
	  (alist-get locality container-property-alist nil nil 'equal)
	  nil nil 'equal)))
    (unless properties
      (error "Could not find properties for '%s' '%s'" program-name locality))
    (format
     "%s%s%s%s"
     (alist-get "access-protocol" properties nil nil 'equal)
     (alist-get "registry" properties  nil nil 'equal)
     basename
     (alist-get "container-suffix" properties nil nil 'equal))))


(defun construct-container-config (template program-name locality)
  (let ((program-name-key
	 (pcase program-name
	   ("podman" "docker")
	   (_ program-name))))
    (format-spec template
	       `((?p . ,program-name)
		 (?G . 
		     ,(construct-container-uri "gladiator"
					       program-name-key
					       locality))
	       (?P .
		   ,(construct-container-uri
		     "pyprophet-legacy"
		     program-name-key
		     locality))))))

(construct-container-config template program-name locality)

#+end_src





#+RESULTS: construct-container-uri
: docker://registry.gitlab.utu.fi/elixirdianf/gladiator-notes/test

#+RESULTS:
: https://registry.gitlab.utu.fi/elixirdianf/gladiator-notes/gladiator
** Per backend config files
:PROPERTIES:
:HEADER-ARGS:nextflow: :mkdirp t :tangle (format  "config/%s.nf" (replace-regexp-in-string "^config-" "" (or (org-element-property :name (org-element-at-point)) ""))) :noweb tangle
:END:
So here we fill the templates that we defined in the previous section.
If you are reading this as an org-mode file,
this headings default tangle argument makes each blocks tangle output file
based on the name of the 

#+NAME: config-singularity
#+begin_src nextflow
<<construct-container-config("singularity","remote")>>
#+end_src
#+NAME: config-singularity-local
#+begin_src nextflow 
<<construct-container-config("singularity","local")>>
#+end_src

#+NAME: config-docker
#+begin_src nextflow 
<<construct-container-config("docker","remote")>>
#+end_src

#+NAME: config-docker-local
#+begin_src nextflow
<<construct-container-config("docker","local")>>
#+end_src

#+NAME: config-podman
#+begin_src nextflow 
<<construct-container-config("podman","remote")>>
#+end_src

#+NAME: config-podman-local
#+begin_src nextflow
<<construct-container-config("podman","local")>>
#+end_src

* Putting it together
** gwl
#+begin_src scheme :noweb no-export
(make-workflow
 (name "my-workflow")
 (processes
  (auto-connect
   (list
    <<gwl-proc>>
    ))))
#+end_src
** nf
according to =nextflow/docs/process.rst=,
the cache looks at the timestamp for cache-correctness,
which can be inconsistent in on shared file systems,
setting =process.cache= to 'lenient' maybe will work around this?
See alos 
#+begin_quote
``'lenient'``         Enable caching. Cache keys are created indexing input files path and size attributes (this policy provides a workaround for incorrect caching invalidation observed on shared file systems due to inconsistent files timestamps; requires version 0.32.x or later).
#+end_quote

there is also 'deep' hashing, based on file content
and the undocumented 'sha256' hashing bashed on the shasum of the file
See also here: https://www.nextflow.io/blog/2019/troubleshooting-nextflow-resume.html

#+begin_src nextflow :noweb-ref nf-params :tangle no
process.cache='lenient'
#+end_src

though this will break if you replace input files with files with the same
path but different content.
'sha256' might be the best one?

The results files will all be output in the following directory
#+begin_src nextflow :noweb-ref nf-params :tangle no
// directory where the  results will be output to 
params.outdir = "./results"
#+end_src




#+begin_src nextflow :tangle nextflow.config :noweb no-export
<<nf-params>>
#+end_src

* File local variables
For this file

# Local Variables:
# compile-command: "cd /data/epouta1/B22003_Elixir_DIA_Nextflow/playground/gladiator-nf && module use /appl/user/modulefiles/ && module load nextflow && salloc nextflow run gladiator.nf"
# End:

For the scheme file

#+begin_src scheme
;; Local Variables;
;; compile-command: "guix shell guix guile gwl --with-git-url=gwl=git://git.savannah.gnu.org/gwl.git --with-commit=gwl=e233be5cf0e2f9cb37e3daa299f5031bea56ba71 -- guix workflow run -v10 gwl-gladiator.scm
;; End:
#+end_src
For the nextflow file
#+begin_src nextflow
// Local Variables:
// compile-command: "module use /appl/user/modulefiles/ && module load nextflow && salloc nextflow run gladiator.nf"
// End:
#+end_src



* Note
:LOGBOOK:
- State "TODO"       from              [2022-06-03 Fri 15:52]
:END:
Pyprophet calling convention altered after v2.1.0,
check what changed
so can't literally copy invocation from =workflow.py=.
(S. said)
* Launching
#+NAME: launch-nextflow
#+begin_example sh :eval no
module use /appl/user/modulefiles/
module load nextflow
mkdir -p runs
RUNLOGDIR=`mktemp -p runs -d run.XXXX`
export TMPDIR=$PWD/tmp
echo $RUNLOGDIR
sbatch --partition=long  --output=$RUNLOGDIR/slurm.log --err=$RUNLOGDIR/slurm.log --time=13-11:59:00 nextflow -log $RUNLOGDIR/.nextflow.log run gladiator.nf -with-trace $RUNLOGDIR/trace.txt -with-report $RUNLOGDIR/report.html -with-dag $RUNLOGDIR/graph.dot --dda_assisted=false --fastafiles='/data/epouta1/B22003_Elixir_DIA_Nextflow/datasets/pxd014194/pxd014194/fasta/*.fasta' --diafiles='/data/epouta1/B22003_Elixir_DIA_Nextflow/datasets/pxd003497/DIA/MZML-pwiz/*.mzML'  --precursor_mass_tolerance=50 --fragment_mass_tolerance=0.1 --osw_use_cache=false -resume -c pxd000672.config 
#+end_example

#+NAME: launch-test
#+begin_example sh
module use /appl/user/modulefiles/
module load nextflow
mkdir -p runs
RUNLOGDIR=`mktemp -p runs -d run.XXXX`
export TMPDIR=$PWD/tmp
echo $RUNLOGDIR
sbatch --output=$RUNLOGDIR/slurm.log --err=$RUNLOGDIR/slurm.logo nextflow -log $RUNLOGDIR/.nextflow.log run gladiator.nf -with-trace $RUNLOGDIR/trace.txt -with-report $RUNLOGDIR/report.html -with-dag $RUNLOGDIR/graph.dot --dda_assisted=false --fastafiles='/data/epouta1/B22003_Elixir_DIA_Nextflow/datasets/pxd014194/pxd014194/fasta/*.fasta' --diafiles='/data/epouta1/B22003_Elixir_DIA_Nextflow/datasets/bruderer/DIA/MZML-pwiz/B_D140314_SGSDSsample1_R01_MHRM_T0.mzML' --protFDR=0.01  --precursor_mass_tolerance=10 --fragment_mass_tolerance=0.02 -- 
#+end_example



#+begin_src emacs-lisp :var nextflow-compile-command=launch-nextflow
(let ((default-directory (concat (file-remote-p default-directory)
				   "/data/epouta1/B22003_Elixir_DIA_Nextflow/playground/gladiator-nf"))
	;; dont ask about saving things
	(compilation-save-buffers-predicate 'ignore))
  (compile nextflow-compile-command))

#+end_src

#+RESULTS:
: #<buffer *compilation*>

* Guixy Containers for nextflow :noexport:

** For tracing

#+NAME: nextflow-trace-deps
#+begin_src scheme :noweb-ref deps :tangle no
("trace"
 "procps"
 "grep"
 ;; core-utils are needed for touch, kill, test,head, tr ...
 ;; see the nxf_trace_linux() function in a nextflow jobs .command.run file
 "coreutils"
 "gawk"
 "sed")
#+end_src

Putting it into a file
#+begin_src scheme :tangle ci/guix/manifests/nextflow-trace.scm :noweb yes
;;; this defines the list of packages that are needed to
;;; run with nextflow -trace options
(specifications->manifest
 (cdr
  (quote 
    <<nextflow-trace-deps>>)))
#+end_src

** summmarize containers
#+begin_src scheme :noweb no-export :results code
(map
 (lambda (x)
   (cons (car x)
   (list
    'specifications->manifest
    (cdr x))))
   
 '(
  <<deps>>
))
#+end_src

#+RESULTS:
#+begin_src scheme
(("join-fasta-files" specifications->manifest ("python" "biopython")) ("generate-pseudo-spectra" specifications->manifest ("dia-umpire" "pwiz")) ("swath2stats" specifications->manifest ("r-minimal" "r-swath2stats" "r-peca" "r-tidyr" "r-argparse" "r-corrplot")) ("trace" specifications->manifest ("procps" "grep" "bash-minimal" "gawk" "sed")))
#+end_src


* for editing :noexport:
To make the file links work,
=git clone= gladiator here.
(Maybe i'll put gladiator as a git submodule?)

#+begin_src emacs-lisp :tangle no :results none
(defun set-local-abbrevs (abbrevs)
  "Add ABBREVS to `local-abbrev-table' and make it buffer local.
ABBREVS should be a list of abbrevs as passed to `define-abbrev-table'.
The `local-abbrev-table' will be replaced by a copy with the new abbrevs added,
so that it is not the same as the abbrev table used in other buffers with the
same `major-mode'."
  (let* ((bufname (buffer-name))
         (prefix (substring (md5 bufname) 0 (length bufname)))
         (tblsym (intern (concat prefix "-abbrev-table"))))
    (set tblsym (copy-abbrev-table local-abbrev-table))
    (dolist (abbrev abbrevs)
      (define-abbrev (eval tblsym)
        (cl-first abbrev)
        (cl-second abbrev)
        (cl-third abbrev)))
    (setq-local local-abbrev-table (eval tblsym))))
(set-local-abbrevs '(
		     ("nfv" ;;variables
		      "#+begin_src nextflow :noweb-ref nf-vars :tangle no\n\n#+end_src"
		      previous-line)
		     ("nfp" ;; params file
		      "#+begin_src nextflow :noweb-ref nf-params :tangle no\n\n#+end_src"
		      previous-line)
		     ("gwv" ;; variables
		      "#+begin_src scheme :noweb-ref gwl-vars :tangle no\n\n#+end_src"
		      previous-line)
		     ("gwp" ;; processes to be autoconnected
		      "#+begin_src scheme :noweb-ref gwl-proc :tangle no\n\n#+end_src"
		      previous-line)
		     ("wfd"
		      "#+begin_src scheme :noweb-ref deps :tangle no \n\n#+end_src"
		      previous-line)))
#+end_src


#+begin_src emacs-lisp 
(defun gladiator-compile-test ()
  (interactive)
  (let ((default-directory (concat (file-remote-p default-directory)
				   "/data/epouta1/B22003_Elixir_DIA_Nextflow/playground/gladiator-nf"))
	;; dont ask about saving things
	(compilation-save-buffers-predicate 'ignore))
    (compile "module use /appl/user/modulefiles/ && module load nextflow && sbatch nextflow -log `mktemp -u XXX.nextflow.log` run gladiator.nf")))
;; -process.container='file://../gladiator/gladiator.simg' -with-singularity -main-script gladiator
(defun gladiator-tangle-and-compile ()
  (interactive)
  (call-interactively 'org-babel-tangle)
  (gladiator-compile-test))

(use-local-map (copy-keymap org-mode-map))
(local-set-key (kbd "C-c m") 'gladiator-tangle-and-compile)
#+end_src

#+RESULTS:
: gladiator-tangle-and-compile

** Exporting to pdf :noexport:
#+begin_src emacs-lisp
(require 'ox-latex)
(let ((org-latex-packages-alist '("newfloat" "minted"))
      (org-latex-listigs 'minted)
      (org-latex-pdf-process
       '("latexmk -shell-escape -f -pdf -%latex -interaction=nonstopmode -output-directory=%o %f"))
      (org-latex-minted-langs
       (cons '(nextflow "groovy")
	     org-latex-minted-langs)))
  (org-latex-export-to-pdf))
#+end_src

** TAGS 
Here we define regexes like etags's tag-file so that
we can ~xref-find-definitions~ (~M-.~) nextflow variables.
#+begin_src text :tangle nextflow.tags
  --
  -- The above line is empty so that org-mode doesnt this line
  -- This is an etags file for nextflow
  -- this matches a process definition
/ *process +\([^{ ]+\)/\1/
  -- this matches outputting into a channel
/ *\(file\|env\|path\) +.*into *\([^ ]+\)/\2/
  -- this matches assignment with = 
/ *\([A-Za-z0-9_.-]+\) *=[^=]/\1/
  -- this matches assignment with set({channel_name})
/.*set{\([^}\]+\)}/\1/
#+end_src
** COMMENT dir-locals
Here we set the dir-locals,
this is to be run interactively
especially so that ~org-html-head~ can be inferred from the 
#+begin_src emacs-lisp  :results value file :file .dir-locals.el :eval no-export :tangle no :var print-length='()
(pp
 (list
  (cons 'org-mode
	`(
	  (org-latex-packages-alist . (("newfloat" "minted")))
	  (org-latex-listings . minted)
	  (org-latex-pdf-process "latexmk -shell-escape -f -pdf -%latex -interaction=nonstopmode -output-directory=%o %f")
	  ;; minted knows grooct but not nextflow
	  ;; so we map nextflow to groovy
	  (org-latex-minted-langs
	   (emacs-lisp "common-lisp")
	   (cc "c++")
	   (cperl "perl")
	   (shell-script "bash")
	   (caml "ocaml")
	   (nextflow "groovy"))
	  (org-babel-load-languages
	   ((dot . t)))
	  (org-babel-confirm-evaluate-answer-no . nil)
	  (org-confirm-babel-evaluate . nil)
	  ;; for batch export
	  (org-html-htmlize-output-type . css)
	  (org-export-with-broken-links . t)
	  (org-html-head . ,
			 (with-current-buffer
			     (let ((htmlize-css-name-prefix "org-")
				   (htmlize-output-type 'css)
				   )
			       
			       (htmlize-buffer))
			   (when (re-search-forward "<style" nil t)
			     (delete-region (point-min) (match-beginning 0)))
			   (when (re-search-forward "</style>" nil t)
			     (delete-region (1+ (match-end 0)) (point-max)))
			   (current-buffer)
			   (buffer-string)))))))
#+end_src

#+RESULTS:
[[file:.dir-locals.el]]



